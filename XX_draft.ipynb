{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkCHccyuG_Kg"
      },
      "source": [
        "# Assignment overview <ignore>\n",
        "The overarching goal of this assignment is to produce a research report in which you implement, analyse, and discuss various Neural Network techniques. You will be guided through the process of producing this report, which will provide you with experience in report writing that will be useful in any research project you might be involved in later in life.\n",
        "\n",
        "All of your report, including code and Markdown/text, ***must*** be written up in ***this*** notebook. This is not typical for research, but is solely for the purpose of this assignment. Please make sure you change the title of this file so that XXXXXX is replaced by your candidate number. You can use code cells to write code to implement, train, test, and analyse your NNs, as well as to generate figures to plot data and the results of your experiments. You can use Markdown/text cells to describe and discuss the modelling choices you make, the methods you use, and the experiments you conduct. So that we can mark your reports with greater consistency, please ***do not***:\n",
        "\n",
        "* rearrange the sequence of cells in this notebook.\n",
        "* delete any cells, including the ones explaining what you need to do.\n",
        "\n",
        "If you want to add more code cells, for example to help organise the figures you want to show, then please add them directly after the code cells that have already been provided. \n",
        "\n",
        "Please provide verbose comments throughout your code so that it is easy for us to interpret what you are attempting to achieve with your code. Long comments are useful at the beginning of a block of code. Short comments, e.g. to explain the purpose of a new variable, or one of several steps in some analyses, are useful on every few lines of code, if not on every line. Please do not use the code cells for writing extensive sentences/paragraphs that should instead be in the Markdown/text cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAAMLJvMBOnn"
      },
      "source": [
        "# Abstract/Introduction (instructions) - 15 MARKS <ignore>\n",
        "Use the next Markdown/text cell to write a short introduction to your report. This should include:\n",
        "* a brief description of the topic (image classification) and of the dataset being used (CIFAR10 dataset). (2 MARKS)\n",
        "* a brief description of how the CIFAR10 dataset has aided the development of neural network techniques, with examples. (3 MARKS)\n",
        "* a descriptive overview of what the goal of your report is, including what you investigated. (5 MARKS)\n",
        "* a summary of your major findings. (3 MARKS)\n",
        "* two or more relevant references. (2 MARKS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjQqL_Xb5cH1"
      },
      "source": [
        "### Abstract\n",
        "Through structured experimentation this assignment explores and demonstrates a number of fundamental properties of artificial neural networks during training and testing. \n",
        "\n",
        "Using a relatively simple convolutional neural network to classify images in the CIFAR-10 dataset, the effects of network and hyperparamater choices are demonstrated and analysed with a focus not on performance but on clarity of understanding.\n",
        "\n",
        "Many of the results demonstrated the behaviours associated with the interventions employed. However, poor performance by the model using batch normalisation was not expected, although was ultimately understandble given the context.\n",
        "\n",
        "### Introduction\n",
        "The labelled CIFAR-10 dataset and it's larger sibling CIFAR-100 have been used for benchmarking and testing in many exploratory and ground breaking papers relating to computer vision and image classification, not least in the development of Alexnet [8], Resnet [4] and most recently transformer-for vision architectures [3]. It is fitting, then, to use it to explore some of the fundamental properties of aritificial neural networks (NN) in this assignment.\n",
        "\n",
        "The first experiment examined the effect that altering the learning rate (LR) has on training and performance. As well as experimenting with different learning rates, a LR 'scheduler' was designed and its effect on performance analysed through comparison to models with static learning rates.\n",
        "\n",
        "The second experiment aimed to demonstrate and offer insight into the impact of introducing a dropout layer into the arhchitecture of the network. Different dropout rates were trialled and their effects compared to baseline performance during both training and evaluation. The effect of dropout was also tested in a transfer learning context.\n",
        "\n",
        "The third experiment focused on analysing gradient flow during back propagation in different architectures. Gradients were measured and plotted in all layers during training in the baseline model, a model with dropout, and a model with batch normalisation, with results compared to gain insight. The performance of the model training with batch normalisation was also compared to that of the other models.\n",
        "\n",
        "Approaches and methods are introduces in the methodology section, with results and analysis offered afterwards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5aTZVqp5uVv"
      },
      "source": [
        "# Methodology (instructions) - 55 MARKS <ignore>\n",
        "Use the next cells in this Methodology section to describe and demonstrate the details of what you did, in practice, for your research. Cite at least two academic papers that support your model choices. The overarching prinicple of writing the Methodology is to ***provide sufficient details for someone to replicate your model and to reproduce your results, without having to resort to your code***. You must include at least these components in the Methodology:\n",
        "* Data - Decribe the dataset, including how it is divided into training, validation, and test sets. Describe any pre-processing you perform on the data, and explain any advantages or disadvantages to your choice of pre-processing. \n",
        "* Architecture - Describe the architecture of your model, including all relevant hyperparameters. The architecture must include 3 convolutional layers followed by two fully connected layers. Include a figure with labels to illustrate the architecture.\n",
        "* Loss function - Describe the loss function(s) you are using, and explain any advantages or disadvantages there are with respect to the classification task.\n",
        "* Optimiser - Describe the optimiser(s) you are using, including its hyperparameters, and explain any advantages or disadvantages there are to using that optimser.\n",
        "* Experiments - Describe how you conducted each experiment, including any changes made to the baseline model that has already been described in the other Methodology sections. Explain the methods used for training the model and for assessing its performance on validation/test data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ygcFzZZPRgB"
      },
      "source": [
        "## Data (7 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The CIFAR-10 dataset was developed as a labelled subset of the 80 million tiny images dataset [7]. It consists of 60,000 low resolution (32x32) colour images split into 50,000 training examples and 10,000 testing examples. Each image belongs to one 10 mutually exclusive classes and is labelled accordingly. These classes describe the suject of the image ('airplane', 'cat', 'ship', etc.).\n",
        "\n",
        "It is conveniently accessable, along with many other benchmarking datasets, via the Pytorch `datasets` method which enables the user to load both training and test data into separate `torch.Dataset` instances extremely easily, and this was the method used here. \n",
        " \n",
        "As part of this process it is possible to apply manual transforms to the data as it is loaded and here the data was both converted to tensors and standrdised using this approach. The standardisation (so that the pixel values in the 3 input channels had a mean of 0 and and a standard deviation of 1) ensured and the model would learn only the informative variation in the data.  \n",
        "\n",
        "The training instances were split to create a validation set of 5000 samples (with a random seed set for consistency across experiments). The class distribution for for each dataset was found to be well balanced (see Fig 1) meaning simple accuracy will be a reliable measure of overall performance across the classes.\n",
        "\n",
        "<figure><center><img src=\"./figs/classdisttraining.png\" width=200><img src=\"./figs/classdistval.png\" width=200><img src=\"./figs/class dist test.png\" width=200><figcaption style=\"max-width: 600px\"> Figure 1. Class distributions across the training, validation, and testing datasets</figcaption></center></figure>\n",
        "\n",
        "Data Batching for stochastic gradient descent was handled by the `DataLoader` class, which yields samples without replacement from the shuffled dataset in batches of a size that can be specified by the user.\n",
        "\n",
        "It was decided that a single train and validation split would be appropriate for the task at hand. Cross-validation was discounted as the benefit of a more accurate idea of the likely performance of the model, or exposure to absolutely all of the possible training data was not an important consideration here. This is because the objective is to understand the minutiae of model behaviour rather than to maximise final performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW5bfoqFlAkI"
      },
      "source": [
        "## Architecture (17 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<figure><center><img src=\"./figs/baseline_model_diagram.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 2. BaselineNet Convolutional Neural Network architecture. </figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/TABLE.PNG\" width=600><figcaption style=\"max-width: 600px\"> Table 1: Convolutional Neural Network Architecture</figcaption></center></figure>\n",
        "\n",
        "The choices for the initial architecture were based on a combination of the assignment brief, initial experimentation, and common practices in the field.\n",
        "\n",
        "Fig 2. shows the overall arhcitecture of the model, whilst table 1 shows the detail of the convolutional layers. There were a number of considerations that went into these choices. \n",
        "\n",
        "Filter dimensions of 3x3 were chosen as they have been shown to be effective in capturing local spatial patterns while keeping the number of parameters relatively low. Indeed, VGG net demonstrated the power of stacked 3x3 filter-based convolutional layers [14], and although they were used in a much deeper network there, that network was classifying much higher resolution images.\n",
        "\n",
        "The increasing number of filters in the convolutional layers allows the network to learn progressively more complex and abstract features as the depth increases, and was another property shown to be effective in the VGG network [14]. \n",
        "\n",
        "Setting the stride and padding to 1 in the convolutional layers ensured that the spatial resolution was preserved, while preventing information loss at the edges.\n",
        "\n",
        "The max pooling layers all have a pool size of 2x2 and stride of 2. This reduces the spatial dimensions and therefore reduces the number of parameters but it also and provides a form of translation invariance because the exact position of a feature within the 2x2 window becomes less important; the max pooling layer only keeps the maximum activation value within each window.\n",
        "\n",
        "A batch size of 64 was selected as a balance between computational efficiency and the ability to capture a representative sample of the dataset in each iteration.\n",
        "\n",
        "The choice of size for the fully connected layer was a balance between the capacity requirements of the model and the number of paramaters that could realistically be trained over numerous runs during the experiments. As `fc1` takes as its input the <lt>$1024$</lt> activations from the flattened convolutional layer before, the weights of this layer are <lt>$1024*d$</lt> where $d$ is the dimensionality of `fc1`  The final value of <lt>$64$</lt> outputs resulted in <lt>$65,536$</lt> trainable paramters which was a good compromise.\n",
        "\n",
        "ReLU was chosen for the non-linear activation throughout for the same reasons it is often chosen, namely its ability to avoid vanishing gradients owing to the fact it does not saturate as other activations such as sigmoid or tanh do, and so it avoids values close to 0 on differentiation during back propogation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Og6OEUtIPT-P"
      },
      "source": [
        "## Loss function (3 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss function used for each experiment was cross-entropy loss, implimented using the `nn.CrossEntropyLoss` class from Pytorch [9].\n",
        "\n",
        "It is widely used in classification problems such as this where the target variable is binomial or miultinomial. \n",
        "\n",
        "It works by first transforming the raw logits of the output layer into what is a effectively a probability distribution via the softmax activation function. Where <lt>$C$</lt> is the number of classes, it outputs is  $C$-dimensional vector of real numbers in the range (0, 1) that sum to 1.\n",
        "\n",
        "To calculate the loss this distribution is compared to a one-hot encoded version of the true class label. This acts as a target probability distribution for the comparison and the cross entropy loss calculation essentially quantifies the difference via the following calculation.  \n",
        "\n",
        "For a single sample with true label <lt>$y$</lt> and predicted probabilities <lt>$\\hat{y}$</lt>, the cross-entropy loss is calculated as:\n",
        "<lt>$$\\text{CE}(y, \\hat{y}) = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$</lt>\n",
        "\n",
        "where <lt>$y_i$</lt> is the true label (0 or 1) for class <lt>$i$</lt>, and <lt>$\\hat{y}_i$</lt> is the predicted probability for class <lt>$i$</lt> as given by the softmax output. \n",
        "\n",
        "By minimizing the average cross-entropy loss over all training samples, the model learns to assign high probabilities to the correct class and low probabilities to the incorrect ones.\n",
        "\n",
        "Practically, the Pytorch module precludes the need for a softmax layer in the model architecture itself as the optimiser takes in the raw logits and then applys the `nn.LogSoftmax()` activation function [11] and the `nn.NLLLoss()` [12] (Negative Log-Likelihood Loss) in a single operation that encapsulates the above. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh2Zgcj9PXAl"
      },
      "source": [
        "## Optimiser (4 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The optmiser used to handle parameter updates and impliment gradient descent was stochastic gradient descent (SGD), implimented using the `optim.SGD` class from Pytorch. \n",
        "\n",
        "SGD estimates the true gradient of the loss function with respect to the paramaters of the model by calulating the gradient of a small subset of the training data (a mini-batch) and updates the parameters of the model with this approximate gradient, weighted by a LR which - in this approach - is fixed, and is a user defined hyperparamater that can be tuned. \n",
        "\n",
        "This process is repeated for multiple mini-batch samples taken from the training data without replacement (until the entire data set has been seen - representing an 'epoch' of training) and then repeated until a stopping criterion is met - in this case a set number of epochs.\n",
        "\n",
        "Mathematically, the estimated gradient for a mini-batch of size $B$ sampled from the training data is computed as:\n",
        "<lt>$$\\nabla_\\theta L(\\theta_t) \\approx \\frac{1}{B} \\sum_{i=1}^{B} \\nabla_\\theta L(\\theta_t; x_i, y_i)$$</lt>\n",
        "where <lt>$(x_i, y_i)$</lt> represents the <lt>$i$</lt>-th example in the mini-batch.\n",
        "\n",
        "A number of more sophiticated optimisers are available when training NNs today, however as performance was not the chief consideration, SGD was chosen to make analysing the impact of LR on performance straightforward and transparent. With SGD paramaters are directly updated based only on the gradient and the learning rate. By keeping to this very direct forumlation it easier to understand and interpret the impact of the LR on the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4J8SM1rMyReN"
      },
      "source": [
        "## Experiments <ignore>\n",
        "### Experiment 1 (8 MARKS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.1 - Learning Rates\n",
        "A number of exploratory training runs of different LRs between <lt>$0.5$</lt> and <lt>$1e^-6$</lt> were carried out to established the extremes of model behaviour. A top value above which learning would be unstable, and a low value below which no learning would occur were found ($0.15$ and $0.001$ respectively). \n",
        "\n",
        "These findings suggested the range from which to select the 5 LRs to compare for the experiment which were chosen as <lt>$0.1, 0.075. 0,05, 0.025 \\text{, and }0.01$</lt>\n",
        "\n",
        "For all trials the data was loaded and processed as described above using the model, criterion and optimiser specified. \n",
        "\n",
        "For each learning rate, 5 trials were conducted. That is; 5 different models were instantiated, trained and evlauated. The results for each of these trials were recorded. The 5 trials were run by iterating over a list of random seeds that was kept constant for all trials in all experiments to allow for comparison and consistency in weight initialisation, dropout activation selection and all other random processes.  \n",
        "\n",
        "Models were trained by mini-batch stoachastic gradient descent as described above. During training each batch was scored in terms of loss and accuracy where loss was as above and accuracy was a simply count of how many images were correctly classified divided by the number of images in the batch.\n",
        "\n",
        "Batch scores were averaged across the epoch to give the training loss and accuracy for that epoch. After each epoch of training, the model was taken out of training mode - halting gradient computations - and the validation scores were calculated by iterating through the validation data in batches. At the end of training each model was then evaluated against the test dataset. In order to obtain the 'test score' for each LR as shown in Fig 4, the average of the 5 models instantiated for that LR was taken. Test scores were calculated as validation scores were. \n",
        "\n",
        "Accumulating these metrics across epochs rather than batches is a somewhat aribitraty although conventional approach. It is a convenient way to keep track of how many times the model has been exposed to all of the training data and is easy to understand when plotting performance graphs. \n",
        "\n",
        "The data for each learning rate's performance was stored in a JSON file for later plotting and analysis. \n",
        "\n",
        "#### 1.2 - LR Scheduler\n",
        "\n",
        "Having established above the performance of different LRs it was clear that the model could tolerate a relatively high initial LR but that this needed to drop significantly and arrive at or beneath 0.02 by the end of the 50 epochs to ensure a more fine grained exploration of the loss landscape in later stages. \n",
        "\n",
        "A number of approaches to LR scheduling were explored visually as in Fig 5. below. The function and decay rate that best fit the finding of experiment 1.1 was 'inverse time decay' with a decay rate of 0.25. This function is defined as <lt>$\\alpha_t = \\frac{\\alpha_0}{1 + kt}$</lt> where <lt>$\\alpha_t$</lt> is the LR at time step <lt>$t$</lt>, <lt>$\\alpha_0$</lt> is the initial learning rate, <lt>$k$</lt> is the decay rate, <lt>$t$</lt> is the current time step or iteration. \n",
        "\n",
        "How this function modifies the LR over the epochs can be seen in plot 1 of Fig 5.\n",
        "\n",
        "A model was then trained as above using this LR decay function which was applied evey epoch. The results of this training were gathered and plotted as can be seen in Fig 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 2 (8 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.1 - Dropout Rates\n",
        "For this experiment the original training data was re-split into two halves to create new training and validation datasets of 25,000, each and a new model was defined encorporating dropout in the fully connected layers.\n",
        "\n",
        "Of the two fully connected layers in the model, one is connected to the output layer and would not typically have dropout applied (as these connections are directly outputting the logits used for classification). Through flattening the final convolutional layer is in a sense fully connected to `fc1`. However, it is not generally a good idea to apply dropout to CNN activations, as it can disrupt the spatial structure and correlation in the feature map representations. It was decided then to use a single dropout layer applied to the activations of only `fc1`.\n",
        "\n",
        "The set of dropout rates for experimentation was defined as <lt>$0, 0.2, 0.4, 0.6, \\text{, and }0.8$</lt> as 0 had to be included and 1 would mean no activation were passed forward at all.\n",
        "\n",
        "The same approach as in experiment 1.1 was taken to training and validation, with 5 trials carried out for each dropout rate with models initialised with consistent seeding. \n",
        "\n",
        "The experiment's results (seen in Fig 9) show the effect of dropout rgularisation on model performance, and Fig 11 established the optimal dropout rate for the DropoutNet model on this specific classification task - which was 0.6. \n",
        "\n",
        "#### 2.2 - Dropout and Transfer Leaning\n",
        "\n",
        "The second part of this expeiment investigated the performance of dropout regularization in the context of transfer learning.\n",
        "\n",
        "It compared the performance of the best performing model from experiment 1 with:\n",
        "*i)* a model pretrained on the original data *without* dropout then retrained on the new data\n",
        "*ii)* a model pretrained don the original data *with** dropout then retrained on the new data. \n",
        "\n",
        "In both of the latter cases the retaining was partial and amounted to transfer learning where pretrained models had some weights 'frozen' whilst others were reintialised and made trainable on the new data.\n",
        "\n",
        "Transfer learning was implimented as follows.\n",
        "\n",
        "Two models, one with dropout, one without, were initialised and trained as in previous experiments, iterating over 5 random seeds gathering training, validation and testing performance data. The final instance of each model was saved to disk so the trained models weights were stored.\n",
        "\n",
        "The validation and training datasets were then swapped, the models were loaded and all of their layers were frozen except their fully connected layers which were manually re-initialised, meaning they were subject to training. \n",
        "\n",
        "These two models were then trained on the new, swapped data as in previous experiments. By the end of this process these models (in their different layers) had effectively been exposed to two slightly differently distributed datasets - pretrained on the original data, and then their final layers had been trained on the new data. \n",
        "\n",
        "Their performance during this retraining on training, validation and testing data was recorded. \n",
        "\n",
        "The averaged results and smoothed plots seen in Figs 14, 16, 17 and 18 provide insights into how the pretrained models with and without dropout perform when fine-tuned on the swapped data. The test results on the original test dataset assess the models' performance on unseen data and can be compared to other models. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 3 (8 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Gradient Flow Analysis\n",
        "\n",
        "#### 3.1, 3.2, 3.3\n",
        "\n",
        "This experiment investigated gradient flow in different networks. It compared the previously seen models with a model with batch normalisation implimented in order to discover any differences in how gradient propogates through these different arhcitectures. \n",
        "\n",
        "For this experiment a new model implimenting batch normalisation was defined using the inbuilt Python `nn.BatchNorm`. This impliments a process by which the activations of a layer are normalised by subtracting the batch mean and dividing by the batch standard deviation. After normalisation, the activations are scaled and shifted using learnable parameters (`bn.weight` and `bn.bias` in Fig 23.). It has been shown to enable faster learning rates and to have a regularising effect among other benefits [1], [6]. It was applied to all except for the last layer here. \n",
        "\n",
        "The process for each of the baseline, dropout and batchnormalised models was the same and was as follows. \n",
        "\n",
        "The gradient for each layer in each model was gathered and averaged across the first 5 episodes and the last 5 episodes during training. PyTorch conveniently makes these values accessible as a property of the model, and all that was required was to collect and calculate the averages for each layer across the correct episodes.\n",
        "\n",
        "Training was carried out over 30 epochs. The original data split was re-instigated, and a fixed LR of 0.05 was selected. \n",
        "\n",
        "For each model the same random seed was initialised, then the model, criterion and loss initialised as in previous experiments. During training, rather than gathering performance data, the gradient data was collected as described above. \n",
        "\n",
        "This data was then plotted in a variety of forms to highlight the trends in the data. \n",
        "\n",
        "It should be noted that rather than the raw gradient values being collected, it was the absolute values. The reasons for this are made clear in the results section for this experiment. \n",
        "\n",
        "#### 3.4\n",
        "\n",
        "Finally, a batch normalised model was trained on the original data for 50 epochs as in previous experiments, with performance on the original training, validation and tes datasets recorded and plotted. It was compared and analysed in relation to other models performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "############################################\n",
        "### Code for building the baseline model ###\n",
        "############################################\n",
        "\n",
        "# relevant imports\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F # as per convention\n",
        "\n",
        "class BaselineNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # max pool layers - not strictly needed to be seperate instances but helps with reference to the diagram\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=64 * 4 * 4, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l4idTCtPfbc"
      },
      "source": [
        "# Results (instructions) - 55 MARKS <ignore>\n",
        "Use the Results section to summarise your findings from the experiments. For each experiment, use the Markdown/text cell to describe and explain your results, and use the code cell (and additional code cells if necessary) to conduct the experiment and produce figures to show your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "batSfx6_oUOk"
      },
      "source": [
        "### Experiment 1 (17 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.1\n",
        "As can be seen in Fig 1, initial experminetation established reasonable limits within which to select LRs for further testing. Rates of 0.15 and above lead to unusual, erratic behaviour such as that seen in Fig 1.2 where the LR is so high that the model cannot converge to an optimal solution and instead overshoots. On the other hand 1.1 shows the other extreme where the LR is so low no learning can occur.\n",
        "\n",
        "<figure><center><img src=\"./figs/e1/lrchaos.png\" width=700><img src=\"./figs/e1/tranval_no_learn.png\" width=700><figcaption style=\"max-width: 600px\"> Fig 1. Showing behavioural extremes for different learning rates: unstable learning at a LR of 0.2, and minimal learning at a LR of 0.001 </figcaption></center></figure>\n",
        "\n",
        "The performances of different LRs can be seen in Fig 2. below and are well summarised in Fig 3. Looking at Fig 2., it can be seen that as LRs get smaller the generalisation gap between the training and validation loss and accuracy is slower to develop, and less extreme. This shows that models trained with higher LRs are able to fit to the training data more quickly, but also overfit to it more quickly. The impact of the LR can also be seen the volatility in of the training loss which is markablty lower at lower learning rates.\n",
        "\n",
        "<figure><center><img src=\"./figs/e1//lr1.png\" width=700><img src=\"./figs/e1/lr2.png\" width=700><img src=\"./figs/e1/lr3.png\" width=700><img src=\"./figs/e1/lr4.png\" width=700><img src=\"./figs/e1/lr5.png\" width=700><figcaption style=\"max-width: 600px\"> Fig 2. Performance plots showing individual and averaged training and validation losses and accuracies for models trained with descending LRs across 50 epochs of training </figcaption></center></figure>\n",
        "\n",
        "In terms of performance on unseen data, the test performances in Fig 4. and the smoothed validation losses and accuracies in Fig 3. give a good overview of how LRs affect this, with lower LRs leading to a reduced validition loss at the end of the 50 epochs owing to slower fitting (and thus overfitting), but also lower accuracy in test and validation. This was in contrast to the quicker rise to high accuracy for those with high learning rates,followed by a plateaing and gradual decline. \n",
        "\n",
        "<figure><center><img src=\"./figs/e1/smoothed loss accuracy.png\" width=700><figcaption style=\"max-width: 600px\"> Fig 3. Smoothed averaged results for accuracies and losses across 50 epochs on validation data for models trained with different learning rates</figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/e1/leraning rates test performance.PNG\" width=300><figcaption style=\"max-width: 600px\"> Fig 4. Test set performance of models trained with different LRs highlighting the best result for each metric in green</figcaption></center></figure>\n",
        "\n",
        "#### 1.2\n",
        "\n",
        "Having observed the different performances above, it was clear the ideal balance would be a LR that began at the highest end of the LRs above (0.1), but that decayed reasonably quickly in order to avoid the onset of overiftting around 10 epochs. \n",
        "\n",
        "Different approaches to decay and how they affect LR over the 50 epochs can be seen in Fig 5. The smooth inverse time fuinction with a decay rate of 0.25 seemed to have the ideal combination and was found to perform well relative to the others. \n",
        "\n",
        "<figure><center><img src=\"./figs/e1/lr_scheculer experiments.png\" width=350><figcaption style=\"max-width: 600px\"> Fig 5. Different LR decay schedules affect on the active LR across 50 epochs </figcaption></center></figure>\n",
        "\n",
        "As can be seen when a model using this shceduler is compared with a model using a static LR (see fig 1.3), there is a slight improvment in overall performance with a shceduler, although however the most substantial difference appears to be in the stability of the validation loss and accuracies despite seeming to over fit to the training data. The LR scheduled model's validation accuracy stabalises in a way that did not occur with any of the  the other models that saturated at close to 100% training accuracy before. This is likely because the even decreasing LR means that after a certain point the paramaters will settle as they will only be getting the negligable updates. \n",
        "\n",
        "<figure><center><img src=\"./figs/e1/LR SCHEDULER final results.png\" width=700><figcaption style=\"max-width: 600px\"> Fig 6. Performance over 50 epochs of training for model trained with LR scheduler </figcaption></center></figure>\n",
        "<figure><center><img src=\"./figs/e1/results accuracy camparison lr and scheduler.png\" width=350><figcaption style=\"max-width: 600px\"> Fig 7. Comparison of performance across training of model trained with a LR scheduler, and the best performing model without a scheduler (LR of 0.05)</figcaption></center></figure>\n",
        "<figure><center><img src=\"./figs/e1/lr decay comparison.PNG\" width=400><figcaption style=\"max-width: 600px\"> Fig 8. Comparison of test results between a model trained with a LR scheduler, and the best performing model trained without a scheduler (LR of 0.05) highlighting the best result for each metric in green</figcaption></center></figure>\n",
        "\n",
        "That this model achieves 100% on the training set is noteable in itself - something which none of the earlier models did. This is again likely owing to (in the case of high LR models) being too coarse to hone in on a particular point in paramater space that would give it 100% accuracy, or (in the case of the very low learning rates) possibly being unable to tranverse the loss landscape effectively owing to too small a gradient, possibly getting stuck in sub-optimal minima.  \n",
        "\n",
        "Overall this experiment demonstrates well the impact that different LRs can have on learning in a NN model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BiBjSG9ioDPR"
      },
      "outputs": [],
      "source": [
        "\n",
        "#############################\n",
        "### code for Experiment 1 ###\n",
        "#############################\n",
        "\n",
        "# UTIL functions that are used here and in all other experiments are included at the bottom of this cell. \n",
        "# This choice was made so the experiment code came first to help with readability\n",
        "# it does mean some function calls show as undefined\n",
        "\n",
        "\n",
        "# imports \n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import math\n",
        "\n",
        "# use GPU where available\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "# EXPERIMENT 1.1 ------------- LRs -------------\n",
        "\n",
        "# DATA LOADING AND SPLITTING\n",
        "\n",
        "# set seed for data split\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# create transform object so conversion to Tensor and normalising carried out on data download (functionality as part of torchvision.datasets method)\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# get the data - 'train' boolean specifies whether to get training or test data\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
        "\n",
        "# set value for validation split (10% validation)\n",
        "num_validation_samples = 5000\n",
        "num_train_samples = len(train_data) - num_validation_samples\n",
        "\n",
        "# split training data\n",
        "train_data, val_data = random_split(train_data, [num_train_samples, num_validation_samples])\n",
        "\n",
        "# confirm split number\n",
        "print(len(train_data)) # 50000 training egs  \n",
        "print(len(val_data)) # 10000 test egs\n",
        "print(len(test_data)) # 10000 test egs\n",
        "\n",
        "# set batch side for initialising dataloaders intialise for different datasets\n",
        "batch_size = 64\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# RUNNING TRAINING AND VALIDATIOB\n",
        "\n",
        "num_epochs = 50\n",
        "random_seeds = list(range(1, 6))\n",
        "\n",
        "learning_rates_for_experiment = [0.1, 0.075, 0.05, 0.025, 0.01]\n",
        "# initialise dictionary for storing data for saving to JSON\n",
        "averaged_results = {lr:{} for lr in learning_rates_for_experiment}\n",
        "path_to_save = f'./run_data/learning_rates/FINAL.json'\n",
        "path_to_load = f'./run_data/learning_rates/FINAL.json'\n",
        "save_experiment = True\n",
        "# iterate over LRs to be tested\n",
        "for learning_rate in learning_rates_for_experiment:\n",
        "    # initialise empty lists for collecting data for each LRs (over the 5 runs)\n",
        "    epoch_train_losses_by_run = []\n",
        "    epoch_val_losses_by_run = []\n",
        "    epoch_train_accuracies_by_run = []\n",
        "    epoch_val_accuracies_by_run = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    reports = []\n",
        "    \n",
        "    # 5 random seeds = 5 different runs for each learning rate\n",
        "    for random_seed in random_seeds:\n",
        "        # set seed prior to initialising model (as used for initial weights as well as any dropout layers)\n",
        "        torch.manual_seed(random_seed)\n",
        "        # initialise model, criterion and optimiser\n",
        "        model = BaselineNet().to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimiser = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "        \n",
        "        model, train_epoch_losses, train_epoch_accuracy, val_epoch_losses, val_epoch_accuracy, _,_ = run_training_and_validation(model, device, learning_rate, num_epochs, criterion, optimiser, train_dataloader, val_dataloader, manual_lr_schedule=False, plot=True)\n",
        "        epoch_train_losses_by_run.append(train_epoch_losses)\n",
        "        epoch_val_losses_by_run.append(val_epoch_losses)\n",
        "        epoch_train_accuracies_by_run.append(train_epoch_accuracy)\n",
        "        epoch_val_accuracies_by_run.append(val_epoch_accuracy)\n",
        "        \n",
        "        test_loss, test_accuracy, report = run_testing(model, device, criterion, test_dataloader)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        reports.append(report)\n",
        "    \n",
        "    average_train_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_train_losses_by_run)]\n",
        "    average_val_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_val_losses_by_run)]\n",
        "    average_train_accuracies = [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_train_accuracies_by_run)]\n",
        "    average_val_accuracies =  [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_val_accuracies_by_run)]\n",
        "    \n",
        "    average_test_loss = sum(test_losses)/len(test_losses)\n",
        "    average_test_accuracy = sum(test_accuracies)/len(test_accuracies)\n",
        "    \n",
        "    averaged_results[learning_rate] = {'seeds':random_seeds,\n",
        "                                       'av_train_losses': average_train_losses,\n",
        "                                       'av_val_losses': average_val_losses,\n",
        "                                       'av_train_acc': average_train_accuracies,\n",
        "                                       'av_val_acc': average_val_accuracies,\n",
        "                                       'all_train_losses':epoch_train_losses_by_run,\n",
        "                                       'all_val_losses': epoch_val_losses_by_run,\n",
        "                                       'all_train_accuracies': epoch_train_accuracies_by_run,\n",
        "                                       'all_val_accuracies': epoch_val_accuracies_by_run,\n",
        "                                       'all_test_losses':test_losses, \n",
        "                                       'all_test_accuracies':test_accuracies,\n",
        "                                       'av_test_loss': average_test_loss,\n",
        "                                       'av_test_accuracy':average_test_accuracy}\n",
        "    plot_single_train_val_smoothed(average_train_losses,average_val_losses,average_train_accuracies,average_val_accuracies, num_epochs, smoothing_window=5, title=f'lr: {learning_rate}')\n",
        "\n",
        "if save_experiment:\n",
        "    with open(path_to_save, 'w') as file:\n",
        "        json.dump(averaged_results, file, indent=4)  # 'indent' makes the output formatted and easier to read\n",
        "\n",
        "\n",
        "# PLOTTING\n",
        "\n",
        "plot_all_models_performance_from_disk(path_to_load, enforce_axis=True)\n",
        "plot_performance_comparison_from_file(path_to_load, enforce_axis=True)\n",
        "display_accuracy_heatmap(path_to_load)\n",
        "\n",
        "\n",
        "# EXPERIMENT 1.2 ------------- LR SCHEDULER -------------\n",
        "\n",
        "# INVESTIGATE LR DECAY\n",
        "\n",
        "# exploring different learning_rate decay approaches and plotting them to see how the LRwill actually behave across 50 epochs\n",
        "def adjust_learning_rate(epoch, initial_lr, decay_type, decay_rate=0.1, decay_interval=10):\n",
        "    if decay_type == 'inverse_time':\n",
        "        new_lr = initial_lr / (1 + decay_rate * epoch)\n",
        "    elif decay_type == 'exponential':\n",
        "        new_lr = initial_lr * (math.e ** (-1 * decay_rate * epoch))\n",
        "    elif decay_type == 'step':\n",
        "        num_decays = epoch // decay_interval\n",
        "        new_lr = initial_lr * (decay_rate ** num_decays)\n",
        "    return new_lr\n",
        "\n",
        "def plot_learning_rate_decay(num_epochs, initial_lr, decay_functions):\n",
        "    fig, axs = plt.subplots(len(decay_functions), figsize=(8, 4 * len(decay_functions)))\n",
        "    if len(decay_functions) == 1:\n",
        "        axs = [axs]\n",
        "    \n",
        "    for i, (decay_type, decay_rate, decay_interval) in enumerate(decay_functions):\n",
        "        lr_values = [adjust_learning_rate(epoch, initial_lr, decay_type, decay_rate, decay_interval) for epoch in range(num_epochs)]\n",
        "        \n",
        "        if decay_type == 'step':\n",
        "            title = f'Decay Function: {decay_type}, Decay Rate: {decay_rate}, Decay Interval: {decay_interval}'\n",
        "        else:\n",
        "            title = f'Decay Function: {decay_type}, Decay Rate: {decay_rate}'\n",
        "        \n",
        "        axs[i].plot(range(num_epochs), lr_values)\n",
        "        axs[i].set_title(title)\n",
        "        axs[i].set_xlabel('Epoch')\n",
        "        axs[i].set_ylabel('Learning Rate')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "num_epochs = 50\n",
        "initial_lr = 0.1\n",
        "\n",
        "decay_functions = [\n",
        "    ('inverse_time', 0.1, 0),\n",
        "    ('inverse_time', 0.05, 0),\n",
        "    ('step', 0.5, 10),\n",
        "    ('step', 0.1, 5),\n",
        "    ('exponential', 0.25, 0),\n",
        "    ('exponential', 0.1, 0)\n",
        "]\n",
        "\n",
        "plot_learning_rate_decay(num_epochs, initial_lr, decay_functions)\n",
        "\n",
        "\n",
        "# RUN TRAINING AND VALIDATION WITH LRDECAY\n",
        "\n",
        "# implimenting the most LR decay shceduler that best fit what I wanted to happen\n",
        "# creating function that will be passed in to the training function to be applied after evey epoch\n",
        "def adjust_initial_learning_rate(optimiser, epoch, initial_lr=0.1, decay_rate=0.25):    \n",
        "    new_lr = initial_lr / (1 + decay_rate *epoch)\n",
        "    for param_group in optimiser.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "    print('LR:',new_lr)\n",
        "    return optimiser\n",
        "\n",
        "\n",
        "num_epochs = 50\n",
        "\n",
        "initial_learning_rate = 0.1\n",
        "decay_rate = 0.25\n",
        "\n",
        "random_seeds = list(range(1, 6))\n",
        "\n",
        "averaged_results = {decay_rate:{}}\n",
        "path_to_save = f'./run_data/lr_decay/final_decaying_lr_initial_lr_{initial_learning_rate}_decay_{decay_rate}.json'\n",
        "path_to_load = f'./run_data/lr_decay/final_decaying_lr_initial_lr_{initial_learning_rate}_decay_{decay_rate}.json'\n",
        "\n",
        "save_experiment = True\n",
        "\n",
        "epoch_train_losses_by_run = []\n",
        "epoch_val_losses_by_run = []\n",
        "epoch_train_accuracies_by_run = []\n",
        "epoch_val_accuracies_by_run = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "reports = []\n",
        "    \n",
        "for random_seed in random_seeds:\n",
        "    print('DECAY: ', decay_rate)\n",
        "    print('seed:', random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "\n",
        "    model = BaselineNet().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimiser = optim.SGD(model.parameters(), lr=initial_learning_rate)\n",
        "\n",
        "    model,train_epoch_losses, train_epoch_accuracy, val_epoch_losses, val_epoch_accuracy, train_report,val_report = run_training_and_validation(model, device, initial_learning_rate, num_epochs, criterion, optimiser, train_dataloader, val_dataloader, manual_lr_schedule=True, scheduler_func=adjust_initial_learning_rate, plot=True)\n",
        "    epoch_train_losses_by_run.append(train_epoch_losses)\n",
        "    epoch_val_losses_by_run.append(val_epoch_losses)\n",
        "    epoch_train_accuracies_by_run.append(train_epoch_accuracy)\n",
        "    epoch_val_accuracies_by_run.append(val_epoch_accuracy)\n",
        "    \n",
        "    test_loss, test_accuracy, report = run_testing(model, device, criterion, test_dataloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    reports.append(report)\n",
        "\n",
        "    \n",
        "    average_train_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_train_losses_by_run)]\n",
        "    average_val_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_val_losses_by_run)]\n",
        "    average_train_accuracies = [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_train_accuracies_by_run)]\n",
        "    average_val_accuracies =  [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_val_accuracies_by_run)]\n",
        "    average_test_loss = sum(test_losses)/len(test_losses)\n",
        "    average_test_accuracy = sum(test_accuracies)/len(test_accuracies)\n",
        "    \n",
        "    averaged_results[decay_rate] = {'seeds':random_seeds,\n",
        "                                       'av_train_losses': average_train_losses,\n",
        "                                       'av_val_losses': average_val_losses,\n",
        "                                       'av_train_acc': average_train_accuracies,\n",
        "                                       'av_val_acc': average_val_accuracies,\n",
        "                                       'all_train_losses':epoch_train_losses_by_run,\n",
        "                                       'all_val_losses': epoch_val_losses_by_run,\n",
        "                                       'all_train_accuracies': epoch_train_accuracies_by_run,\n",
        "                                       'all_val_accuracies': epoch_val_accuracies_by_run,\n",
        "                                       'all_test_losses':test_losses, \n",
        "                                       'all_test_accuracies':test_accuracies,\n",
        "                                       'av_test_loss': average_test_loss,\n",
        "                                       'av_test_accuracy':average_test_accuracy}\n",
        "    \n",
        "    plot_single_train_val_smoothed(average_train_losses,average_val_losses,average_train_accuracies,average_val_accuracies, num_epochs, smoothing_window=3, title=f'LR: {initial_learning_rate}, DECAY: {decay_rate}')\n",
        "    \n",
        "if save_experiment:\n",
        "    with open(path_to_save, 'w') as file:\n",
        "        json.dump(averaged_results, file, indent=4)  # 'indent' makes the output formatted and easier to read\n",
        "\n",
        "# PLOTTING\n",
        "lr_decay_data = path_to_load\n",
        "plot_all_models_performance_from_disk(lr_decay_data, enforce_axis=True)\n",
        "plot_performance_comparison_from_file(lr_decay_data, enforce_axis=True)\n",
        "display_accuracy_heatmap(lr_decay_data)\n",
        "\n",
        "\n",
        "# ---------UTILITY FUNCTIONS USED ACROSS ALL EXPERIMENTS---------\n",
        "\n",
        "# These functions comprised a utils.py file during development\n",
        "\n",
        "# MODEL RELATED (click to expand) :\n",
        "def run_training_and_validation(model, device, initial_lr, num_epochs, criterion, optimiser, train_dataloader, val_dataloader, metrics = False, manual_lr_schedule = False, scheduler_func=None, plot = False):\n",
        "\n",
        "    # key function which performs training and validation of a model for params and data. \n",
        "    \n",
        "    # returns all of the data gathered from the training and validation run organised by epoch. Optional params added during development to accomodate different experiments (eg lr_scheduling)\n",
        "    \n",
        "    # optional metrics and plot paramaters allow for plotting as well as generation of classification report used for analysis of results\n",
        "    \n",
        "    # when plotting, includes a call to plot_single_train_val_smoothed() util function defined below\n",
        "    # when training includes a call to the get_accuracy() function below\n",
        "\n",
        "    train_epoch_losses = []\n",
        "    train_epoch_accuracy = []\n",
        "    val_epoch_losses = []\n",
        "    val_epoch_accuracy = []\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        train_running_batch_losses = []\n",
        "        train_running_batch_accuracy = []\n",
        "        \n",
        "        if epoch == num_epochs-1:\n",
        "            train_all_preds = []\n",
        "            train_all_labels = []\n",
        "            val_all_preds = []\n",
        "            val_all_labels = []\n",
        "        \n",
        "        if manual_lr_schedule:\n",
        "            optimiser = scheduler_func(optimiser, epoch, initial_lr)\n",
        "\n",
        "        model.train()\n",
        "        for i, (images, labels) in enumerate(train_dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            \n",
        "            accuracy = get_accuracy(outputs, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            train_running_batch_losses.append(loss.item())\n",
        "            train_running_batch_accuracy.append(accuracy)\n",
        "            # if i % 50 == 0:\n",
        "            #   training_progress_bar.set_description(f'Training Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_dataloader)}], Loss: {loss.item():.4f}, Acc: {accuracy:.4f}')\n",
        "            \n",
        "            if epoch == num_epochs-1:\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                train_all_preds.extend(preds.cpu().numpy())  # Move predictions to CPU and convert to numpy for sklearn\n",
        "                train_all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and convert to numpy\n",
        "\n",
        "        train_epoch_losses.append(sum(train_running_batch_losses)/len(train_running_batch_losses))\n",
        "        train_epoch_accuracy.append(sum(train_running_batch_accuracy)/len(train_running_batch_accuracy))\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_running_batch_losses = []\n",
        "            val_running_batch_accuracy = []\n",
        "\n",
        "            for i, (images, labels) in enumerate(val_dataloader):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                accuracy = get_accuracy(outputs, labels)\n",
        "\n",
        "                val_running_batch_losses.append(loss.item())\n",
        "                val_running_batch_accuracy.append(accuracy)\n",
        "                # if i % 20 == 0:\n",
        "                #   val_progress_bar.set_description(f'Validation Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(val_dataloader)}], Loss: {loss.item():.4f}, Acc: {accuracy:.4f}')\n",
        "                \n",
        "                if epoch == num_epochs-1:\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    val_all_preds.extend(preds.cpu().numpy())  # Move predictions to CPU and convert to numpy for sklearn\n",
        "                    val_all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and convert to numpy\n",
        "\n",
        "            val_epoch_losses.append(sum(val_running_batch_losses)/len(val_running_batch_losses))\n",
        "            val_epoch_accuracy.append(sum(val_running_batch_accuracy)/len(val_running_batch_accuracy))\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_epoch_losses[epoch]:.4f}, Acc: {train_epoch_accuracy[epoch]:.4f} | Val Loss: {val_epoch_losses[epoch]:.4f}, Acc: {val_epoch_accuracy[epoch]:.4f}')\n",
        "            class_names = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "            \n",
        "    if plot:\n",
        "        plot_single_train_val_smoothed(train_epoch_losses, val_epoch_losses, train_epoch_accuracy, val_epoch_accuracy, num_epochs, smoothing_window=10, title=f'single run lr={initial_lr}, decay={manual_lr_schedule}')\n",
        "    \n",
        "    if metrics:\n",
        "        train_report = classification_report(train_all_labels, train_all_preds, target_names=(class_names))\n",
        "        val_report = classification_report(val_all_labels, val_all_preds, target_names=(class_names))\n",
        "        # print('FINAL EPOCH TRAINING SUMMARY:')\n",
        "        # print(train_report)\n",
        "        # print('FINAL EPOCH VALIDATION SUMMARY:')\n",
        "        # print(val_report)\n",
        "        \n",
        "        return (model,train_epoch_losses, train_epoch_accuracy, val_epoch_losses, val_epoch_accuracy, train_report,val_report)\n",
        "    else:\n",
        "        return (model, train_epoch_losses, train_epoch_accuracy, val_epoch_losses, val_epoch_accuracy, 0,0)\n",
        "\n",
        "def get_accuracy(logits, targets):\n",
        "    \n",
        "        # key function used in all training and valdation and testing runs to calculate the accuracy of predictions made by a model using.\n",
        "        \n",
        "        # takes in logits (raw output scores from the model) and targets (actual class labels) and returns a float representing the accuracy of the predictions.\n",
        "\n",
        "        # get the indices of the maximum value of all elements in the input tensor (which are the predicted class labels)\n",
        "        _, predicted_labels = torch.max(logits, 1)\n",
        "        \n",
        "        # calculate the number of correctly predicted labels.\n",
        "        correct_predictions = (predicted_labels == targets).sum().item()\n",
        "        \n",
        "        # calculate the accuracy.\n",
        "        accuracy = correct_predictions / targets.size(0)\n",
        "        \n",
        "        return accuracy\n",
        "\n",
        "def run_testing(model, device, criterion, test_dataloader):\n",
        "    # this function was used to test trained models on the test dataset\n",
        "    # its returns loss accuracy and the classification report for analysis\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_running_batch_losses = []\n",
        "        test_running_batch_accuracy = []\n",
        "        test_all_preds = []\n",
        "        test_all_labels = []\n",
        "\n",
        "        for i, (images, labels) in enumerate(test_dataloader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            accuracy = get_accuracy(outputs, labels)\n",
        "\n",
        "            test_running_batch_losses.append(loss.item())\n",
        "            test_running_batch_accuracy.append(accuracy)\n",
        "            # test_progress_bar.set_description(f'testidation Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(test_dataloader)}], Loss: {loss.item():.4f}, Acc: {accuracy:.4f}')\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_all_preds.extend(preds.cpu().numpy())  # Move predictions to CPU and convert to numpy for sklearn\n",
        "            test_all_labels.extend(labels.cpu().numpy())  # Move labels to CPU and convert to numpy\n",
        "\n",
        "    test_loss = sum(test_running_batch_losses)/len(test_running_batch_losses)\n",
        "    test_accuracy = sum(test_running_batch_accuracy)/len(test_running_batch_accuracy)\n",
        "\n",
        "    print('TESTING COMPLETE!!')\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.4f}')\n",
        "    report = classification_report(test_all_labels, test_all_preds, target_names=(['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']))\n",
        "    print(report)\n",
        "    return test_loss, test_accuracy, report\n",
        "\n",
        "# PLOTTING/VISUALISING RELATED (click to expand):\n",
        "def plot_single_train_val_smoothed(train_epoch_losses, val_epoch_losses, train_epoch_accuracy, val_epoch_accuracy, num_epochs, smoothing_window=5, title=None):\n",
        "    # function used in many contexts to plot training and validation losses and accuracies of a single run\n",
        "    # takes in the values returne from a single run of training and validation and plots them \n",
        "    # smoothing param allows for clearer picture of the progress during validation especially as it can be volatile \n",
        "    \n",
        "    # convert lists to pandas Series\n",
        "    train_epoch_losses_series = pd.Series(train_epoch_losses)\n",
        "    val_epoch_losses_series = pd.Series(val_epoch_losses)\n",
        "    train_epoch_accuracy_series = pd.Series(train_epoch_accuracy)\n",
        "    val_epoch_accuracy_series = pd.Series(val_epoch_accuracy)\n",
        "\n",
        "    # calculate moving averages using the provided smoothing window\n",
        "    smooth_train_epoch_losses = train_epoch_losses_series.rolling(window=smoothing_window).mean()\n",
        "    smooth_val_epoch_losses = val_epoch_losses_series.rolling(window=smoothing_window).mean()\n",
        "    smooth_train_epoch_accuracy = train_epoch_accuracy_series.rolling(window=smoothing_window).mean()\n",
        "    smooth_val_epoch_accuracy = val_epoch_accuracy_series.rolling(window=smoothing_window).mean()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Plot training and validation loss with moving averages\n",
        "    ax[0].plot(train_epoch_losses, label='Training Loss', alpha=0.3)\n",
        "    ax[0].plot(val_epoch_losses, label='Validation Loss', alpha=0.3)\n",
        "    ax[0].plot(smooth_train_epoch_losses, label='Smoothed Training Loss', color='blue')\n",
        "    ax[0].plot(smooth_val_epoch_losses, label='Smoothed Validation Loss', color='orange')\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].set_title('Training and Validation Loss')\n",
        "    ax[0].legend()\n",
        "\n",
        "    # Set x-axis to show each epoch as a tick\n",
        "    ax[1].set_xticks(range(0, num_epochs + 1, 10))\n",
        "\n",
        "    # Plot training and validation accuracy with moving averages\n",
        "    ax[1].plot(train_epoch_accuracy, label='Training Accuracy', alpha=0.3)\n",
        "    ax[1].plot(val_epoch_accuracy, label='Validation Accuracy', alpha=0.3)\n",
        "    ax[1].plot(smooth_train_epoch_accuracy, label='Smoothed Training Accuracy', color='blue')\n",
        "    ax[1].plot(smooth_val_epoch_accuracy, label='Smoothed Validation Accuracy', color='orange')\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[1].set_ylabel('Accuracy')\n",
        "    ax[1].set_title('Training and Validation Accuracy')\n",
        "    ax[1].legend()\n",
        "\n",
        "    # Set x-axis to show each epoch as a tick\n",
        "    ax[1].set_xticks(range(0, num_epochs + 1, 10))\n",
        "\n",
        "    # Set y-axis for accuracy to range from 0 to 1 with ticks at intervals of 0.1\n",
        "    ax[1].set_ylim(0, 1)\n",
        "    ax[1].set_yticks([i * 0.1 for i in range(11)])\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def display_accuracy_heatmap(path_to_load):\n",
        "    # helper function for displaying best performing models in a convenient way\n",
        "    with open(path_to_load, 'r') as file:\n",
        "        results = json.load(file)\n",
        "    \n",
        "    rates = []\n",
        "    av_test_losses = []\n",
        "    av_test_accuracy = []\n",
        "    for rate, value_dict in results.items():\n",
        "        rates.append(rate)\n",
        "        av_test_losses.append(value_dict['av_test_loss'])\n",
        "        av_test_accuracy.append(value_dict['av_test_accuracy'])\n",
        "    \n",
        "    # Creating the DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'Average Test Loss': av_test_losses,\n",
        "        'Average Test Accuracy': av_test_accuracy\n",
        "    }, index=rates)\n",
        "    \n",
        "    # Applying conditional formatting to highlight the best value in each column\n",
        "    def highlight_best(column):\n",
        "        if column.name == 'Average Test Loss':\n",
        "            is_best = column == column.min()\n",
        "        else:\n",
        "            is_best = column == column.max()\n",
        "        return ['background: green' if v else '' for v in is_best]\n",
        "    \n",
        "    styled_df = df.style.apply(highlight_best, axis=0)\n",
        "    \n",
        "    return styled_df\n",
        "\n",
        "def plot_single_model_performance(single_var_multi_run_data, title=None, enforce_axis=False):\n",
        "    # function used for plotting the performance of single variable being investigated of n multiple runs \n",
        "    # for example during experiments 1.1 and 2.1\n",
        "    \n",
        "    # plots individual runs in background and a clearer average run \n",
        "    \n",
        "    epochs = range(1, len(single_var_multi_run_data['av_train_losses']) + 1)\n",
        "    n_runs = len(single_var_multi_run_data['all_train_losses'])\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    if title:\n",
        "        title += f' across {n_runs} runs'\n",
        "        fig.suptitle(title, fontsize=12)\n",
        "\n",
        "    # Plot losses\n",
        "    for train_loss, val_loss in zip(single_var_multi_run_data['all_train_losses'], single_var_multi_run_data['all_val_losses']):\n",
        "        ax1.plot(epochs, train_loss, color='blue', alpha=0.3, linewidth=0.5, label='Individual Run Training Losses')\n",
        "        ax1.plot(epochs, val_loss, color='orange', alpha=0.3, linewidth=0.5, label='Individual Run Validation Losses')\n",
        "    ax1.plot(epochs, single_var_multi_run_data['av_train_losses'], color='blue', linewidth=1.2, label='Average Training Loss')\n",
        "    ax1.plot(epochs, single_var_multi_run_data['av_val_losses'], color='orange', linewidth=1.2, label='Average Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Losses')\n",
        "    \n",
        "    # Remove duplicate labels in the legend\n",
        "    handles, labels = ax1.get_legend_handles_labels()\n",
        "    unique_labels = [\"Average Training Loss\", \"Average Validation Loss\", \"Individual Run Training Losses\", \"Individual Run Validation Losses\"]\n",
        "    unique_handles = [handles[labels.index(label)] for label in unique_labels]\n",
        "    ax1.legend(unique_handles, unique_labels)\n",
        "\n",
        "    # Plot accuracies\n",
        "    for train_acc, val_acc in zip(single_var_multi_run_data['all_train_accuracies'], single_var_multi_run_data['all_val_accuracies']):\n",
        "        ax2.plot(epochs, train_acc, color='blue', alpha=0.3, linewidth=0.5, label='Individual Run Training Accuracies')\n",
        "        ax2.plot(epochs, val_acc, color='orange', alpha=0.3, linewidth=0.5, label='Individual Run Validation Accuracies')\n",
        "    ax2.plot(epochs, single_var_multi_run_data['av_train_acc'], color='blue', linewidth=1.2, label='Average Training Accuracy')\n",
        "    ax2.plot(epochs, single_var_multi_run_data['av_val_acc'], color='orange', linewidth=1.2, label='Average Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Accuracies')\n",
        "    \n",
        "    # Remove duplicate labels in the legend\n",
        "    handles, labels = ax2.get_legend_handles_labels()\n",
        "    unique_labels = [\"Average Training Accuracy\", \"Average Validation Accuracy\", \"Individual Run Training Accuracies\", \"Individual Run Validation Accuracies\"]\n",
        "    unique_handles = [handles[labels.index(label)] for label in unique_labels]\n",
        "    ax2.legend(unique_handles, unique_labels)\n",
        "    \n",
        "    if enforce_axis:\n",
        "        ax1.set_ylim(0, 5)\n",
        "        ax2.set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()    \n",
        "    \n",
        "def plot_all_models_performance_from_disk(path_to_load, variable_name=None, enforce_axis=False):\n",
        "    with open(path_to_load, 'r') as file:\n",
        "        averaged_results = json.load(file)\n",
        "        \n",
        "    for variable_val, data in averaged_results.items():\n",
        "        plot_single_model_performance(data, title=f'Training/Validation Losses and Accuracy for {variable_name} = {variable_val} across', enforce_axis=enforce_axis)\n",
        "\n",
        "def plot_performance_comparison_from_file(path_to_load, enforce_axis=False, smooth_window=5):\n",
        "    with open(path_to_load, 'r') as file:\n",
        "        results = json.load(file)\n",
        "    learning_rates = list(results.keys())\n",
        "    num_epochs = len(results[learning_rates[0]]['av_train_losses'])\n",
        "\n",
        "    fig_size = (12, 16)\n",
        "    fig, ((ax_train_loss, ax_train_acc), (ax_val_loss, ax_val_acc),\n",
        "          (ax_train_loss_smoothed, ax_train_acc_smoothed),\n",
        "          (ax_val_loss_smoothed, ax_val_acc_smoothed)) = plt.subplots(4, 2, figsize=fig_size)\n",
        "\n",
        "    plot_metrics(ax_train_loss, results, learning_rates, num_epochs, 'av_train_losses', 'Average Training Loss')\n",
        "    plot_metrics(ax_train_acc, results, learning_rates, num_epochs, 'av_train_acc', 'Average Training Accuracy')\n",
        "    plot_metrics(ax_val_loss, results, learning_rates, num_epochs, 'av_val_losses', 'Average Validation Loss')\n",
        "    plot_metrics(ax_val_acc, results, learning_rates, num_epochs, 'av_val_acc', 'Average Validation Accuracy')\n",
        "    plot_metrics(ax_train_loss_smoothed, results, learning_rates, num_epochs, 'av_train_losses', 'Smoothed Training Loss', smoothed=True, smooth_window=smooth_window)\n",
        "    plot_metrics(ax_train_acc_smoothed, results, learning_rates, num_epochs, 'av_train_acc', 'Smoothed Training Accuracy', smoothed=True, smooth_window=smooth_window)\n",
        "    plot_metrics(ax_val_loss_smoothed, results, learning_rates, num_epochs, 'av_val_losses', 'Smoothed Validation Loss', smoothed=True, smooth_window=smooth_window)\n",
        "    plot_metrics(ax_val_acc_smoothed, results, learning_rates, num_epochs, 'av_val_acc', 'Smoothed Validation Accuracy', smoothed=True, smooth_window=smooth_window)\n",
        "\n",
        "    if enforce_axis:\n",
        "        for ax in [ax_val_acc, ax_val_loss, ax_train_acc, ax_train_loss,\n",
        "                   ax_val_acc_smoothed, ax_val_loss_smoothed, ax_train_acc_smoothed, ax_train_loss_smoothed]:\n",
        "            ax.set_ylim(0, 5) if 'Loss' in ax.get_ylabel() else ax.set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    if len(learning_rates) > 2:\n",
        "        plot_comparative_metrics(results, learning_rates, num_epochs, 'Comparative Accuracies', 'av_train_acc', 'av_val_acc', enforce_axis)\n",
        "        plot_comparative_metrics(results, learning_rates, num_epochs, 'Comparative Accuracies (Smoothed)', 'av_train_acc', 'av_val_acc', enforce_axis, smoothed=True, smooth_window=smooth_window)\n",
        "    elif len(learning_rates) == 2:\n",
        "        fig_acc_two, ax_acc_two = plt.subplots(figsize=(6, 4))\n",
        "        fig_acc_two.suptitle('Comparative Accuracies', fontsize=12)\n",
        "\n",
        "        for lr in learning_rates:\n",
        "            ax_acc_two.plot(range(1, num_epochs + 1), results[lr]['av_val_acc'], label=f\"Validation ({lr})\", linestyle='-')\n",
        "            ax_acc_two.plot(range(1, num_epochs + 1), results[lr]['av_train_acc'], label=f\"Training ({lr})\", linestyle='--')\n",
        "\n",
        "        ax_acc_two.set_xlabel('Epoch')\n",
        "        ax_acc_two.set_ylabel('Accuracy')\n",
        "        ax_acc_two.set_title('Accuracy Comparison')\n",
        "        ax_acc_two.legend(loc='upper right')\n",
        "\n",
        "        if enforce_axis:\n",
        "            ax_acc_two.set_ylim(0, 1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        plot_comparative_metrics(results, learning_rates, num_epochs, 'Comparative Accuracies (Smoothed)', 'av_train_acc', 'av_val_acc', enforce_axis, smoothed=True, smooth_window=smooth_window)\n",
        "\n",
        "def plot_metrics(ax, results, learning_rates, num_epochs, metric_key, title, smoothed=False, smooth_window=5):\n",
        "    for lr in learning_rates:\n",
        "        if smoothed:\n",
        "            metric = np.convolve(results[lr][metric_key], np.ones(smooth_window) / smooth_window, mode='valid')\n",
        "            ax.plot(range(smooth_window // 2, num_epochs - smooth_window // 2 + 1), metric, label=str(lr))\n",
        "        else:\n",
        "            ax.plot(range(1, num_epochs + 1), results[lr][metric_key], label=str(lr))\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel(title)\n",
        "    ax.set_title(title)\n",
        "    ax.legend(title='Learning Rates', loc='lower right')\n",
        "\n",
        "def plot_comparative_metrics(results, learning_rates, num_epochs, fig_title, train_key, val_key, enforce_axis=False, smoothed=False, smooth_window=5):\n",
        "    fig, (ax_train, ax_val) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    fig.suptitle(fig_title, fontsize=12)\n",
        "\n",
        "    plot_metrics(ax_train, results, learning_rates, num_epochs, train_key, f'Training {fig_title}', smoothed, smooth_window)\n",
        "    plot_metrics(ax_val, results, learning_rates, num_epochs, val_key, f'Validation {fig_title}', smoothed, smooth_window)\n",
        "\n",
        "    if enforce_axis:\n",
        "        ax_train.set_ylim(0, 1)\n",
        "        ax_val.set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqW3G7sRokZt"
      },
      "source": [
        "### Experiment 2 (19 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.1\n",
        "The effect of increasing dropout rates can clearly be seen in figs 9.1 to 9.5. All models were initialised with the same paramaters other than the dopout rate and what can be observed demonstrates the effect of dropout as a regularisation technique - as the dropout rate increases from 0 to 0.8 we see a reduction in the speed and extent to which the model fits to the training data. This is reflected in the final accuracy it obtains on the training data and the speed with which is gets there. It can also be seen in the significant decrease in the the generalisation gap between training and validation performance, where in the absence of dropout (9.1) there is the biggest gap. and the highest dropout leads to the smallest gap.\n",
        "\n",
        " <figure><center><img src=\"./figs/e2/dr0.png\" width=800><img src=\"./figs/e2/dr02.png\" width=800><img src=\"./figs/e2/dr04.png\" width=800><img src=\"./figs/e2/dr046.png\" width=800><img src=\"./figs/e2/dr08.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 9. Performance plots showing individual and averaged training and validation losses and accuracies for models trained with increasing dropout rates across 50 epochs of training </figcaption></center></figure>\n",
        "<figure><center><img src=\"./figs/e2/overall dropout comparisons.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 10. Smoothed averaged results for accuracies and losses across 50 epochs on validation data for models trained with different dropout rates </figcaption></center></figure>\n",
        "<figure><center><img src=\"./figs/e2/dropout rates test results.PNG\" width=300><figcaption style=\"max-width: 600px\"> Fig 11. Test set performance of models trained with different dropout rates highlighting the best result for each metric in green</figcaption></center></figure>\n",
        "In Fig 10. the comparative performance of models trained with different dropout rates can be seen clearly. Looking at the validation loss, one can see the onset and of that loss is earlier and it is developmeny more severe for the lower dropout rates. Despite this, accuracy is relatively well preserved as the lower dropout rates still  atttain reasonable performance on both test and validation datasets. That being said, the best test loss and test performance belongs to those models with higher dropout rates, albeit by a small margin.\n",
        "\n",
        "#### 2.2\n",
        "The findings of the above are reiterated in the freshly trained models shown in Fig 13. where we see the model trained without dropout (model 0 in these experiments) demonstrating poor generalisability, and marked over fitting, while the model trained with dropout fits less closely to the training data as seen in its poorer performance on the test accuracy. However, what it does learn is mostly generalised to the validation dataset. These findings are also visable in the comparison plot in Fig 13. \n",
        "\n",
        "<figure><center><img src=\"./figs/e2/pretrained_0.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 12. Performance plots showing individual and averaged training and validation losses and accuracies for Baseline (non-dropout) model (model 0)during trained on the original data over 5figcaptionstyle=figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/e2/pretrained_1.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 13. Performance plots showing individual and averaged training and validation losses and accuracies for model with dropout implimented (model 1) during trained on the original data over 50 epochs. </figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/e2/pretrained comparison.png\" width=400><figcaption style=\"max-width: 600px\"> Fig 14. Direct comparison of performance of averaged and smoothed performance of non-dropout (model 0) and dropout (model 1) models over 50 epochs of training and validation on the original data</figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/e2/pretrained test results.PNG\" width=300><figcaption style=\"max-width: 600px\"> Fig 15. Test set performance of models trained without (0) and with (1) dropout implimented highlighting the best result for each metric in green. </figcaption></center></figure>\n",
        "\n",
        "Both of the models above then had their fully connected layers retrained on a reversed version of the original dataset (and so essentially a 'new' dataset in terms of a new distribution) whilst their other parameters remained frozen. Their performance during this second phase of training can be seen in Figs 16 and 17.  \n",
        "\n",
        "It is clear that the dropout-free model fits and then overfits to this new data extremely quickly, with only a very brief period during which it is learning generalisable information from the new data. The model pre-trained and retrained with dropout on the other hand still overfits but the process is smoother and more gradual with a less sever transition between fitting with generalisation to overfitting. \n",
        "\n",
        "\n",
        "\n",
        "<figure><center><img src=\"./figs/e2/retrained_baseline.png\" width=800><figcaption width=500> Fig 16. Performance plots showing individual and averaged training and validation losses and accuracies for Baseline (non-dropout) model (model 0) during retraining on swapped data over 50 epochs</figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/e2/retrained_dropout.png\" width=800><figcaption style=\"max-width: 600px\">Fig 17. Performance plots showing individual and averaged training and validation losses and accuracies for model with dropout (model 0) during retraining on swapped data over 50 epochs</figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/e2/retrained_comparison.png\" width=400><figcaption style=\"max-width: 600px\">Fig 18. Direct comparison of performance of averaged and smoothed performance of non-dropout (model 0) and dropout (model 1) models over 50 epochs of training and validation on the swapped data </figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/e2/retrained comparison test results.PNG\" width=300><figcaption style=\"max-width: 600px\"> Fig 19. Test set performance of models retrained on swapped dataset having been previously trained on original dataset without (0) and with (1) dropout implimented highlighting the best result for each metric in green.</figcaption></center></figure>\n",
        "\n",
        "In terms of overall performance on the test set, as can be seen in Fig 19., the model with dropout performs better on both metrics, although not enormously better. It also performs better than any other model so far other than that which used the LR scheduler. \n",
        "\n",
        "It can therefore be said that the regularisation effect of a single dropout layer was able to improve performance almost to the same level that basic LR scheduling was."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "25000\n",
            "25000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "#############################\n",
        "### Code for Experiment 2 ###\n",
        "#############################\n",
        "\n",
        "# --- EXPERIMENT 2.1 - Dropout Rates ---\n",
        "\n",
        "# DATA LOADING AND NEW SPLIT\n",
        "torch.manual_seed(0)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
        "\n",
        "# half and half split\n",
        "num_validation_samples = 25000\n",
        "num_train_samples = len(train_data) - num_validation_samples\n",
        "train_data, val_data = random_split(train_data, [num_train_samples, num_validation_samples])\n",
        "\n",
        "print(len(train_data)) # 50000 training egs  \n",
        "print(len(val_data)) # 25000 test egs\n",
        "print(len(test_data)) # 10000 test egs\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# DROPOUT MODEL DEFINITION\n",
        "\n",
        "class DropoutNet(nn.Module):\n",
        "    def __init__(self, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(in_features=64 * 4 * 4, out_features=64)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)  # Dropout layer after the first FC layer\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Applying dropout after activation\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# TRAINING WITH DIFFERENT DROPOUT RATES\n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = 0.05\n",
        "\n",
        "random_seeds = list(range(1, 6))\n",
        "dropout_rates_for_experiment = [0, 0.2, 0.4, 0.6, 0.8]\n",
        "\n",
        "averaged_results = {dr:{} for dr in dropout_rates_for_experiment}\n",
        "\n",
        "path_to_save = f'./run_data/dropout/C2_final_dropout_rate_compatison_lr_{learning_rate}_{num_epochs}_epochs.json'\n",
        "path_to_load = f'./run_data/dropout/C2_final_dropout_rate_compatison_lr_{learning_rate}_{num_epochs}_epochs.json'\n",
        "save_experiment = True\n",
        "\n",
        "\n",
        "for dropout_rate in dropout_rates_for_experiment:\n",
        "    print('DR: ', dropout_rate) \n",
        "    epoch_train_losses_by_run = []\n",
        "    epoch_val_losses_by_run = []\n",
        "    epoch_train_accuracies_by_run = []\n",
        "    epoch_val_accuracies_by_run = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    reports = []\n",
        "    \n",
        "    for random_seed in random_seeds:\n",
        "        print('DR: ', dropout_rate) \n",
        "        print('seed:', random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        \n",
        "        model = DropoutNet(dropout_rate).to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimiser = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        model, train_epoch_losses, train_epoch_accuracy, val_epoch_losses, val_epoch_accuracy, _,_ = run_training_and_validation(model, device, learning_rate, num_epochs, criterion, optimiser, train_dataloader, val_dataloader, metrics = False, manual_lr_schedule=False, plot=True)\n",
        "        epoch_train_losses_by_run.append(train_epoch_losses)\n",
        "        epoch_val_losses_by_run.append(val_epoch_losses)\n",
        "        epoch_train_accuracies_by_run.append(train_epoch_accuracy)\n",
        "        epoch_val_accuracies_by_run.append(val_epoch_accuracy)\n",
        "        \n",
        "        test_loss, test_accuracy, report = run_testing(model, device, criterion, test_dataloader)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        reports.append(report)\n",
        "        \n",
        "    average_train_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_train_losses_by_run)]\n",
        "    average_val_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_val_losses_by_run)]\n",
        "    average_train_accuracies = [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_train_accuracies_by_run)]\n",
        "    average_val_accuracies =  [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_val_accuracies_by_run)]\n",
        "    average_test_loss = sum(test_losses)/len(test_losses)\n",
        "    average_test_accuracy = sum(test_accuracies)/len(test_accuracies)\n",
        "    \n",
        "    averaged_results[dropout_rate] = {'seeds':random_seeds,'av_train_losses': average_train_losses,\n",
        "                                       'av_val_losses': average_val_losses,\n",
        "                                       'av_train_acc': average_train_accuracies,\n",
        "                                       'av_val_acc': average_val_accuracies,\n",
        "                                       'all_train_losses':epoch_train_losses_by_run,\n",
        "                                       'all_val_losses': epoch_val_losses_by_run,\n",
        "                                       'all_train_accuracies': epoch_train_accuracies_by_run,\n",
        "                                       'all_val_accuracies': epoch_val_accuracies_by_run,\n",
        "                                       'all_test_losses':test_losses, \n",
        "                                       'all_test_accuracies':test_accuracies,\n",
        "                                       'av_test_loss': average_test_loss,\n",
        "                                       'av_test_accuracy':average_test_accuracy}\n",
        "    print('average for ')\n",
        "    print('DR: ', dropout_rate) \n",
        "    plot_single_train_val_smoothed(average_train_losses,average_val_losses,average_train_accuracies,average_val_accuracies, num_epochs, smoothing_window=3, title=f'DROPOUT: {dropout_rate}')\n",
        "\n",
        "if save_experiment:\n",
        "    with open(path_to_save, 'w') as file:\n",
        "        json.dump(averaged_results, file, indent=4)  # 'indent' makes the output formatted and easier to read\n",
        "        \n",
        "# PLOTTING\n",
        "dropout_data = path_to_load\n",
        "plot_all_models_performance_from_disk(dropout_data, enforce_axis=True)\n",
        "plot_performance_comparison_from_file(dropout_data, enforce_axis=True)\n",
        "display_accuracy_heatmap(dropout_data)\n",
        "\n",
        "# --- EXPERIMENT 2.1 - TRANSFER LEARNINNG ---\n",
        "\n",
        "# SWAP DATASETS WITH NEW DATALOADERS\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "original_train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "original_val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "swapped_train_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "swapped_val_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# TRAINING ON ORIGINAL DATA\n",
        "\n",
        "# train and save models ready transfer learning \n",
        "# train two models - one dropout, one not dropout, train them on the ORIGINAL half and half data, then save a copy of the models to disk\n",
        "best_dropout_rate = 0.6\n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = 0.05\n",
        "\n",
        "random_seeds = [list(range(1, 6))]\n",
        "\n",
        "\n",
        "path_to_save = f'./run_data/transfer_learning/transfer_learn_original_dat_{num_epochs}_epochs_lr_{learning_rate}.json'\n",
        "path_to_load = f'./run_data/transfer_learning/transfer_learn_original_dat_{num_epochs}_epochs_lr_{learning_rate}.json'\n",
        "\n",
        "models = [0, 1]\n",
        "averaged_results = {i:{} for i in models}\n",
        "\n",
        "save_experiment = True\n",
        "\n",
        "# train them both on the original data\n",
        "for i, model in enumerate(models):\n",
        "    epoch_train_losses_by_run = []\n",
        "    epoch_val_losses_by_run = []\n",
        "    epoch_train_accuracies_by_run = []\n",
        "    epoch_val_accuracies_by_run = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    reports = []\n",
        "    \n",
        "    for random_seed in random_seeds:\n",
        "        print('MODEL: ', i) \n",
        "        print('seed:', random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        \n",
        "        model = BaselineNet() if i == 0 else DropoutNet(dropout_rate=best_dropout_rate)\n",
        "        model.to(device)\n",
        "        \n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimiser = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "        \n",
        "        model, train_epoch_losses, train_epoch_accuracy, val_epoch_losses, val_epoch_accuracy, _,_ = run_training_and_validation(model, device, learning_rate, num_epochs, criterion, optimiser, original_train_dataloader, original_val_dataloader, metrics = False, manual_lr_schedule=False, plot=True)\n",
        "        epoch_train_losses_by_run.append(train_epoch_losses)\n",
        "        epoch_val_losses_by_run.append(val_epoch_losses)\n",
        "        epoch_train_accuracies_by_run.append(train_epoch_accuracy)\n",
        "        epoch_val_accuracies_by_run.append(val_epoch_accuracy)\n",
        "        \n",
        "        test_loss, test_accuracy, report = run_testing(model, device, criterion, test_dataloader)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        reports.append(report)\n",
        "        \n",
        "    average_train_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_train_losses_by_run)]\n",
        "    average_val_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_val_losses_by_run)]\n",
        "    average_train_accuracies = [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_train_accuracies_by_run)]\n",
        "    average_val_accuracies =  [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_val_accuracies_by_run)]\n",
        "    average_test_loss = sum(test_losses)/len(test_losses)\n",
        "    average_test_accuracy = sum(test_accuracies)/len(test_accuracies)\n",
        "    \n",
        "    averaged_results[i] = {'seeds':random_seeds,'av_train_losses': average_train_losses,\n",
        "                                       'av_val_losses': average_val_losses,\n",
        "                                       'av_train_acc': average_train_accuracies,\n",
        "                                       'av_val_acc': average_val_accuracies,\n",
        "                                       'all_train_losses':epoch_train_losses_by_run,\n",
        "                                       'all_val_losses': epoch_val_losses_by_run,\n",
        "                                       'all_train_accuracies': epoch_train_accuracies_by_run,\n",
        "                                       'all_val_accuracies': epoch_val_accuracies_by_run,\n",
        "                                       'all_test_losses':test_losses, \n",
        "                                       'all_test_accuracies':test_accuracies,\n",
        "                                       'av_test_loss': average_test_loss,\n",
        "                                       'av_test_accuracy':average_test_accuracy}\n",
        "    print('average for ')\n",
        "    print('Model: ', i) \n",
        "    plot_single_train_val_smoothed(average_train_losses,average_val_losses,average_train_accuracies,average_val_accuracies, num_epochs, smoothing_window=3, title=f'PRETRAINING MODEL: {i}')\n",
        "    \n",
        "    # save last version of model to disk for retraining    \n",
        "    torch.save(model, f'./models/trained_model_{i}.pth')\n",
        "\n",
        "    \n",
        "if save_experiment:\n",
        "    with open(path_to_save, 'w') as file:\n",
        "        json.dump(averaged_results, file, indent=4)  # 'indent' makes the output formatted and easier to read\n",
        "        \n",
        "# PLOTTING\n",
        "pre_training_data = path_to_load\n",
        "plot_all_models_performance_from_disk(pre_training_data, enforce_axis=True)\n",
        "plot_performance_comparison_from_file(pre_training_data, enforce_axis=True)\n",
        "display_accuracy_heatmap(pre_training_data)\n",
        "\n",
        "\n",
        "# PERFORM TRANSFER LEARNING\n",
        "# load in the two pretrained models and then reinitialise some layers\n",
        "# retrain on the SWAPPED data\n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = 0.05\n",
        "random_seeds = list(range(1,6))\n",
        "\n",
        "path_to_save = f'./run_data/transfer_learning/transfer_learning_data_{num_epochs}_epochs_lr_{learning_rate}.json'\n",
        "path_to_load = f'./run_data/transfer_learning/transfer_learning_data_{num_epochs}_epochs_lr_{learning_rate}.json'\n",
        "\n",
        "models = [0, 1]\n",
        "averaged_results = {i:{} for i in models}\n",
        "\n",
        "save_experiment = True\n",
        "\n",
        "# train them both on the swapped train and val data - test data same\n",
        "for i, model in enumerate(models):\n",
        "    epoch_train_losses_by_run = []\n",
        "    epoch_val_losses_by_run = []\n",
        "    epoch_train_accuracies_by_run = []\n",
        "    epoch_val_accuracies_by_run = []\n",
        "    test_losses = []\n",
        "    test_accuracies = []\n",
        "    reports = []\n",
        "    \n",
        "    for random_seed in random_seeds:\n",
        "        print('MODEL: ', i) \n",
        "        print('seed:', random_seed)\n",
        "        torch.manual_seed(random_seed)\n",
        "        # here handle the loading of saved model and reinitiailisation of the fully connected layers\n",
        "        if i == 0:\n",
        "            pretrained_model_non_dropout = torch.load('./models/trained_model_0.pth')\n",
        "            pretrained_model_non_dropout.fc1 =  nn.Linear(in_features=64 * 4 * 4, out_features=64)\n",
        "            pretrained_model_non_dropout.fc2 = nn.Linear(in_features=64, out_features=10)\n",
        "            model = pretrained_model_non_dropout\n",
        "        elif i == 1:\n",
        "            pretrained_model_best_dropout = torch.load('./models/trained_model_1.pth')\n",
        "            pretrained_model_best_dropout.fc1 =  nn.Linear(in_features=64 * 4 * 4, out_features=64)\n",
        "            pretrained_model_best_dropout.fc2 = nn.Linear(in_features=64, out_features=10)\n",
        "            model = pretrained_model_best_dropout\n",
        "        model.to(device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimiser = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "        model, train_epoch_losses, train_epoch_accuracy, val_epoch_losses, val_epoch_accuracy, _,_ = run_training_and_validation(model, device, learning_rate, num_epochs, criterion, optimiser, swapped_train_dataloader, swapped_val_dataloader, metrics = False, manual_lr_schedule=False, plot=True)\n",
        "        epoch_train_losses_by_run.append(train_epoch_losses)\n",
        "        epoch_val_losses_by_run.append(val_epoch_losses)\n",
        "        epoch_train_accuracies_by_run.append(train_epoch_accuracy)\n",
        "        epoch_val_accuracies_by_run.append(val_epoch_accuracy)\n",
        "        \n",
        "        test_loss, test_accuracy, report = run_testing(model, device, criterion, test_dataloader)\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        reports.append(report)\n",
        "        \n",
        "    average_train_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_train_losses_by_run)]\n",
        "    average_val_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_val_losses_by_run)]\n",
        "    average_train_accuracies = [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_train_accuracies_by_run)]\n",
        "    average_val_accuracies =  [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_val_accuracies_by_run)]\n",
        "    average_test_loss = sum(test_losses)/len(test_losses)\n",
        "    average_test_accuracy = sum(test_accuracies)/len(test_accuracies)\n",
        "    \n",
        "    averaged_results[i] = {'seeds':random_seeds,'av_train_losses': average_train_losses,\n",
        "                                       'av_val_losses': average_val_losses,\n",
        "                                       'av_train_acc': average_train_accuracies,\n",
        "                                       'av_val_acc': average_val_accuracies,\n",
        "                                       'all_train_losses':epoch_train_losses_by_run,\n",
        "                                       'all_val_losses': epoch_val_losses_by_run,\n",
        "                                       'all_train_accuracies': epoch_train_accuracies_by_run,\n",
        "                                       'all_val_accuracies': epoch_val_accuracies_by_run,\n",
        "                                       'all_test_losses':test_losses, \n",
        "                                       'all_test_accuracies':test_accuracies,\n",
        "                                       'av_test_loss': average_test_loss,\n",
        "                                       'av_test_accuracy':average_test_accuracy}\n",
        "    print('average for ')\n",
        "    print('Model: ', i) \n",
        "    plot_single_train_val_smoothed(average_train_losses,average_val_losses,average_train_accuracies,average_val_accuracies, num_epochs, smoothing_window=3, title=f'TRANSFER LEARNING MODEL: {i}')\n",
        "    \n",
        "\n",
        "\n",
        "if save_experiment:\n",
        "    with open(path_to_save, 'w') as file:\n",
        "        json.dump(averaged_results, file, indent=4)  # 'indent' makes the output formatted and easier to read\n",
        "\n",
        "# plotting results\n",
        "\n",
        "transfer_learned_data = path_to_load\n",
        "plot_all_models_performance_from_disk(transfer_learned_data, enforce_axis=True)\n",
        "plot_performance_comparison_from_file(transfer_learned_data, enforce_axis=True)\n",
        "display_accuracy_heatmap(transfer_learned_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experiment 3 (19 MARKS) <ignore>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Figs. 20, 21 and 22 show gradient flow through the different models that have been tested so far plus a third model which has had batch normalisation added. As batcy normalisation brings with it new paramaters and new layers, these have been omitted for easier comparison in Fig 22, although the gradient flow in those layers can be seen in fig 23. \n",
        "\n",
        "The absolute value of the gradient was used for all statistics as it provided a clearer representation of gradient magnitudes at the different layers. Absolute values show the size of gradient regardless of sign which was found to be more useful for trying to visualise the propagation of those gradients through the layers. \n",
        "\n",
        "#### 3.1\n",
        "The result in Fig 20. shows that for the baseline model gradient in the first 5 episodes the gradient is small overall, but virtually non existant in the earlier layers. in the last 5 episodes the gradients are higher overall, but also seem to have developed a different spread with larger gradients in the early layers and smaller gradients in later layers. Variability seems to be in proportion to the size of the gradient. \n",
        "\n",
        "These results indicate that for the baseline model there were intially very small updates being make to parameters primarily in the later layers, with little gradient reaching the earliest layers. By the end of training this has changes significantly and there is more information being passed to the earlier layers.\n",
        "\n",
        "<figure><center><img src=\"./figs/e3/gradients baseline model.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 20. Mean and standard deviation of the gradients of the loss function with respect to the paramaters at each layer of thebaseline model during training. </figcaption></center></figure>\n",
        "\n",
        "#### 3.2\n",
        "Fig 21. shows some marked similarities to Fig 20. indicating similarities in gradient flow between the models with and without dropout. The most significant difference is the magnitude of the gradients, which are higher in both the first and last 5 episodes for the dropout model, though with similar variablity and a very similar pattern of propagation as descibed above.\n",
        "\n",
        "<figure><center><img src=\"./figs/e3/gradients dropout model.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 21. Mean and standard deviation of the gradients of the loss function with respect to the paramaters at each layer of the model with dropout implimented during training. </figcaption></center></figure>\n",
        "\n",
        "#### 3.3\n",
        "The results for gradient propogation in the batch normalised model (Figs 22 and 23) are significantly different. Firstly, all of the bias terms for convolutional layers that have had with batch normalisation applied simply dissapear. This is because as the role of the bias paramater is essentially taken over by the parameters of the batch normalisation layer (as seen in Fig 23.) due to the 'absorbtion of bias' phenomenon in batch normalisation [x, y].\n",
        "\n",
        "In the layers that *are* in common, however, a number of other things are striking. Firstly, the values of the gadients are dramatically higher for all layers in the first 5 episodes which is especially significant for the earlier layers where virtually no gradient was reaching in the un-batch-normalised models. In the last 5 episodes it is broadly similar. \n",
        "\n",
        "The distribution of the gradient is also more consistent with in the batch normalised model. Whereas with non-batch normalised models it very much shifts from being mostly updating later layers to then earlier layers, batch norm  is more evenly distributied (with more to the earlier layers) throughout. \n",
        "\n",
        "<figure><center><img src=\"./figs/e3/gradients batchnorm model (matching others).png\" width=800><figcaption style=\"max-width: 600px\"> Fig 22. Mean and standard deviation of the gradients of the loss function with respect to the paramaters at each layer of the model with dropout implimented during training. Not in this plot the batch normalisation layers and their paramaetyrr gradients are not represented to facilitate comparison with previous models  </figcaption></center></figure>\n",
        "<figure><center><img src=\"./figs/e3/gradients batchnorm model (not matching others).png\" width=1000><figcaption style=\"max-width: 600px\"> Fig 23. Mean and standard deviation of the gradients of the loss function with respect to the paramaters at each layer of the model with dropout implimented during training. batch norm paramater gradients included. </figcaption></center></figure>\n",
        "\n",
        "<figure><center><img src=\"./figs/e3/gradint flow relative metrics.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 24. Comparison grouped by metric. </figcaption></center></figure>\n",
        "\n",
        "#### 3.4 \n",
        "The performance of the batch normalised model can be seen below in Fig 25. and on the test dataset in Fig 26. It can be seen that the model overfits quickly and performs quite poorly on the test data, with quite a substantial instability in the validation peformance. This is perhaps a surprising result given the regularistion effect batch norm is often associated with [x, y], and shall be discussed more in the analysis below, as there are other properties of batch normalisation which may be responsible for this finding. \n",
        "\n",
        "<figure><center><img src=\"./figs/e3/batch norm performance.png\" width=800><figcaption style=\"max-width: 600px\"> Fig 25. Performance plots showing individual and averaged training and validation losses and accuracies for a model with batch normalisation applied and trained on orignal data over 50 epochs. </figcaption></center></figure>\n",
        "<figure><center><img src=\"./figs/e3/batch norm test results.PNG\" width=300><figcaption style=\"max-width: 600px\"> Fig 26. Test performance of model trained with batch normalisation</figcaption></center></figure>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAaa3uOcoQ1W"
      },
      "outputs": [],
      "source": [
        "#############################\n",
        "### Code for Experiment 3 ###\n",
        "#############################\n",
        "\n",
        "# return to original data splits\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform)\n",
        "\n",
        "num_validation_samples = 5000\n",
        "num_train_samples = len(train_data) - num_validation_samples\n",
        "\n",
        "train_data, val_data = random_split(train_data, [num_train_samples, num_validation_samples])\n",
        "\n",
        "print(len(train_data)) # 50000 training egs  \n",
        "print(len(val_data)) # 10000 test egs\n",
        "print(len(test_data)) # 10000 test egs\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# define functions for accumulating gradients\n",
        "\n",
        "def collect_gradients_abs_4(model, dataloader, device, criterion, optimizer, num_epochs):\n",
        "    first_5_episodes_gradients_abs = {name: [] for name, _ in model.named_parameters()}\n",
        "    last_5_episodes_gradients_abs = {name: [] for name, _ in model.named_parameters()}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train().to(device)\n",
        "        for batch_count, (images, labels) in enumerate(dataloader, 1):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            episode_gradients_abs = {}\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.grad is None and param.requires_grad:\n",
        "                    # print('HEHRHE')\n",
        "                    episode_gradients_abs[name] = torch.zeros_like(param.data)\n",
        "                elif param.grad is not None:\n",
        "                    # print('NONONO')\n",
        "                    episode_gradients_abs[name] = torch.abs(param.grad.clone().detach())\n",
        "\n",
        "            if epoch == 0 and batch_count <= 5:\n",
        "                for name, grad_abs in episode_gradients_abs.items():\n",
        "                    first_5_episodes_gradients_abs[name].append(grad_abs)\n",
        "            elif epoch == num_epochs - 1 and batch_count > len(dataloader) - 5:\n",
        "                for name, grad_abs in episode_gradients_abs.items():\n",
        "                    last_5_episodes_gradients_abs[name].append(grad_abs)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "    return first_5_episodes_gradients_abs, last_5_episodes_gradients_abs\n",
        "\n",
        "def compute_gradient_statistics_abs_4(gradients_abs):\n",
        "    mean_gradients_abs = {}\n",
        "    std_gradients_abs = {}\n",
        "    for layer_name, layer_gradients_abs in gradients_abs.items():\n",
        "        layer_gradients_abs = torch.stack(layer_gradients_abs)\n",
        "        mean_gradients_abs[layer_name] = torch.mean(layer_gradients_abs, dim=0)\n",
        "        std_gradients_abs[layer_name] = torch.std(layer_gradients_abs, dim=0)\n",
        "    return mean_gradients_abs, std_gradients_abs\n",
        "\n",
        "def plot_gradient_statistics_abs_4(mean_gradients_first5_abs, std_gradients_first5_abs, mean_gradients_last5_abs, std_gradients_last5_abs, skip_bn=True):\n",
        "    if skip_bn:\n",
        "        # Filter out batch normalization layers\n",
        "        layer_names = [name for name in mean_gradients_first5_abs.keys() if not name.startswith('bn')]\n",
        "    else:\n",
        "        layer_names = list(mean_gradients_first5_abs.keys())\n",
        "\n",
        "    num_layers = len(layer_names)\n",
        "    x = np.arange(num_layers)\n",
        "    width = 0.35\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    fig.suptitle('Gradient Statistics (Absolute Means, Absolute Standard Deviations)', fontsize=16)\n",
        "\n",
        "    # Plot mean absolute gradients\n",
        "    ax1.bar(x - width/2, [torch.mean(mean_gradients_first5_abs[name]).item() for name in layer_names], width, label='First 5 Epochs')\n",
        "    ax1.bar(x + width/2, [torch.mean(mean_gradients_last5_abs[name]).item() for name in layer_names], width, label='Last 5 Epochs')\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(layer_names, rotation=45)\n",
        "    ax1.set_xlabel('Layer')\n",
        "    ax1.set_ylabel('Mean of Absolute Gradients')\n",
        "    ax1.set_title('Mean of Absolute Gradients vs Layer')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Plot standard deviations of absolute gradients\n",
        "    ax2.bar(x - width/2, [torch.mean(std_gradients_first5_abs[name]).item() for name in layer_names], width, label='First 5 Epochs')\n",
        "    ax2.bar(x + width/2, [torch.mean(std_gradients_last5_abs[name]).item() for name in layer_names], width, label='Last 5 Epochs')\n",
        "    ax2.set_xticks(x)\n",
        "    ax2.set_xticklabels(layer_names, rotation=45)\n",
        "    ax2.set_xlabel('Layer')\n",
        "    ax2.set_ylabel('Standard Deviation of Absolute Gradients')\n",
        "    ax2.set_title('Standard Deviation of Absolute Gradients vs Layer')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_model_comparison(first_5_mean_gradients_non_drop, first_5_mean_gradients_dropout, first_5_mean_gradients_bn,\n",
        "                          last_5_mean_gradients_non_drop, last_5_mean_gradients_dropout, last_5_mean_gradients_bn,\n",
        "                          first_5_std_gradients_non_drop, first_5_std_gradients_dropout, first_5_std_gradients_bn,\n",
        "                          last_5_std_gradients_non_drop, last_5_std_gradients_dropout, last_5_std_gradients_bn):\n",
        "    layer_names = [name for name in first_5_mean_gradients_non_drop.keys() if not name.startswith('bn')]\n",
        "    print(layer_names)\n",
        "    print(last_5_mean_gradients_non_drop['conv1.weight'])\n",
        "    num_layers = len(layer_names)\n",
        "    x = np.arange(num_layers)\n",
        "    width = 0.2\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Model Comparison - Gradient Statistics', fontsize=16)\n",
        "\n",
        "    # Plot mean absolute gradients for the first 5 epochs\n",
        "    \n",
        "    print([first_5_mean_gradients_non_drop[name].shape for name in layer_names])\n",
        "    axs[0, 0].bar(x - width, [torch.mean(first_5_mean_gradients_non_drop[name]).item() for name in layer_names], width, label='Non-Dropout')\n",
        "    axs[0, 0].bar(x, [torch.mean(first_5_mean_gradients_dropout[name]).item() for name in layer_names], width, label='Dropout')\n",
        "    axs[0, 0].bar(x + width, [torch.mean(first_5_mean_gradients_bn[name]).item() for name in layer_names], width, label='Batch Norm')\n",
        "    axs[0, 0].set_xticks(x)\n",
        "    axs[0, 0].set_xticklabels(layer_names, rotation=45)\n",
        "    axs[0, 0].set_xlabel('Layer')\n",
        "    axs[0, 0].set_ylabel('Mean of Absolute Gradients')\n",
        "    axs[0, 0].set_title('First 5 Epochs - Mean of Absolute Gradients')\n",
        "    axs[0, 0].legend()\n",
        "    # axs[0, 0].set_ylim(0, 0.04)\n",
        "    \n",
        "\n",
        "    # Plot mean absolute gradients for the last 5 epochs\n",
        "    axs[0, 1].bar(x - width, [torch.mean(last_5_mean_gradients_non_drop[name]).item() for name in layer_names], width, label='Non-Dropout')\n",
        "    axs[0, 1].bar(x, [torch.mean(last_5_mean_gradients_dropout[name]).item() for name in layer_names], width, label='Dropout')\n",
        "    axs[0, 1].bar(x + width, [torch.mean(last_5_mean_gradients_bn[name]).item() for name in layer_names], width, label='Batch Norm')\n",
        "    axs[0, 1].set_xticks(x)\n",
        "    axs[0, 1].set_xticklabels(layer_names, rotation=45)\n",
        "    axs[0, 1].set_xlabel('Layer')\n",
        "    axs[0, 1].set_ylabel('Mean of Absolute Gradients')\n",
        "    axs[0, 1].set_title('Last 5 Epochs - Mean of Absolute Gradients')\n",
        "    axs[0, 1].legend()\n",
        "    # axs[0, 1].set_ylim(0, 0.2)\n",
        "    \n",
        "\n",
        "    # Plot standard deviation of absolute gradients for the first 5 epochs\n",
        "    axs[1, 0].bar(x - width, [torch.mean(first_5_std_gradients_non_drop[name]).item() for name in layer_names], width, label='Non-Dropout')\n",
        "    axs[1, 0].bar(x, [torch.mean(first_5_std_gradients_dropout[name]).item() for name in layer_names], width, label='Dropout')\n",
        "    axs[1, 0].bar(x + width, [torch.mean(first_5_std_gradients_bn[name]).item() for name in layer_names], width, label='Batch Norm')\n",
        "    axs[1, 0].set_xticks(x)\n",
        "    axs[1, 0].set_xticklabels(layer_names, rotation=45)\n",
        "    axs[1, 0].set_xlabel('Layer')\n",
        "    axs[1, 0].set_ylabel('Standard Deviation of Absolute Gradients')\n",
        "    axs[1, 0].set_title('First 5 Epochs - Standard Deviation of Absolute Gradients')\n",
        "    axs[1, 0].legend()\n",
        "\n",
        "    # Plot standard deviation of absolute gradients for the last 5 epochs\n",
        "    axs[1, 1].bar(x - width, [torch.mean(last_5_std_gradients_non_drop[name]).item() for name in layer_names], width, label='Non-Dropout')\n",
        "    axs[1, 1].bar(x, [torch.mean(last_5_std_gradients_dropout[name]).item() for name in layer_names], width, label='Dropout')\n",
        "    axs[1, 1].bar(x + width, [torch.mean(last_5_std_gradients_bn[name]).item() for name in layer_names], width, label='Batch Norm')\n",
        "    axs[1, 1].set_xticks(x)\n",
        "    axs[1, 1].set_xticklabels(layer_names, rotation=45)\n",
        "    axs[1, 1].set_xlabel('Layer')\n",
        "    axs[1, 1].set_ylabel('Standard Deviation of Absolute Gradients')\n",
        "    axs[1, 1].set_title('Last 5 Epochs - Standard Deviation of Absolute Gradients')\n",
        "    axs[1, 1].legend()\n",
        "    # axs[1, 1].set_ylim(0, 0.2)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# set epochs and learning rate\n",
        "# Set epochs and learning rate\n",
        "num_epochs = 50\n",
        "learning_rate = 0.05\n",
        "\n",
        "# 3.1 Gradient flow for the original model\n",
        "torch.manual_seed(1984)\n",
        "non_drop_model = BaselineNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = optim.SGD(non_drop_model.parameters(), lr=learning_rate)\n",
        "first_5_epochs_gradients_abs_non_drop, last_5_epochs_gradients_abs_non_drop = collect_gradients_abs_4(non_drop_model, train_dataloader, device, criterion, optimiser, num_epochs)\n",
        "first_5_mean_gradients_non_drop, first_5_std_gradients_non_drop = compute_gradient_statistics_abs_4(first_5_epochs_gradients_abs_non_drop)\n",
        "last_5_mean_gradients_non_drop, last_5_std_gradients_non_drop = compute_gradient_statistics_abs_4(last_5_epochs_gradients_abs_non_drop)\n",
        "plot_gradient_statistics_abs_4(first_5_mean_gradients_non_drop, first_5_std_gradients_non_drop, last_5_mean_gradients_non_drop, last_5_std_gradients_non_drop)\n",
        "\n",
        "# 3.2 Gradient flow for the model with dropout\n",
        "torch.manual_seed(1984)\n",
        "drop_model = DropoutNet(0.6)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = optim.SGD(drop_model.parameters(), lr=learning_rate)\n",
        "first_5_epochs_gradients_abs_dropout, last_5_epochs_gradients_abs_dropout = collect_gradients_abs_4(drop_model, train_dataloader, device, criterion, optimiser, num_epochs)\n",
        "first_5_mean_gradients_dropout, first_5_std_gradients_dropout = compute_gradient_statistics_abs_4(first_5_epochs_gradients_abs_dropout)\n",
        "last_5_mean_gradients_dropout, last_5_std_gradients_dropout = compute_gradient_statistics_abs_4(last_5_epochs_gradients_abs_dropout)\n",
        "plot_gradient_statistics_abs_4(first_5_mean_gradients_dropout, first_5_std_gradients_dropout, last_5_mean_gradients_dropout, last_5_std_gradients_dropout)\n",
        "\n",
        "# 3.3 Gradient flow for the model with batch normalization\n",
        "# create model with BAtch norm as per brief\n",
        "\n",
        "class BatchNormNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.fc1 = nn.Linear(in_features=64 * 4 * 4, out_features=64)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "torch.manual_seed(1984)\n",
        "bn_model = BatchNormNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimiser = optim.SGD(bn_model.parameters(), lr=learning_rate)\n",
        "first_5_epochs_gradients_abs_bn, last_5_epochs_gradients_abs_bn = collect_gradients_abs_4(bn_model, train_dataloader, device, criterion, optimiser, num_epochs)\n",
        "first_5_mean_gradients_bn, first_5_std_gradients_bn = compute_gradient_statistics_abs_4(first_5_epochs_gradients_abs_bn)\n",
        "last_5_mean_gradients_bn, last_5_std_gradients_bn = compute_gradient_statistics_abs_4(last_5_epochs_gradients_abs_bn)\n",
        "plot_gradient_statistics_abs_4(first_5_mean_gradients_bn, first_5_std_gradients_bn, last_5_mean_gradients_bn, last_5_std_gradients_bn, skip_bn=True)\n",
        "plot_gradient_statistics_abs_4(first_5_mean_gradients_bn, first_5_std_gradients_bn, last_5_mean_gradients_bn, last_5_std_gradients_bn, skip_bn=False)\n",
        "\n",
        "# 3.4 \n",
        "# properly train a batch norm model \n",
        "\n",
        "num_epochs = 50\n",
        "learning_rate = 0.05\n",
        "\n",
        "random_seeds = list(range(1, 6))\n",
        "path_to_save = f'./run_data/batch_norm/batch_norm_{num_epochs}_epochs_LR_{learning_rate}.json'\n",
        "path_to_load = f'./run_data/batch_norm/batch_norm_{num_epochs}_epochs_LR_{learning_rate}.json'\n",
        "averaged_results = {'bn':{}}\n",
        "save_experiment = True\n",
        "\n",
        "# train them both on the original data\n",
        "\n",
        "epoch_train_losses_by_run = []\n",
        "epoch_val_losses_by_run = []\n",
        "epoch_train_accuracies_by_run = []\n",
        "epoch_val_accuracies_by_run = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "reports = []\n",
        "\n",
        "for random_seed in random_seeds:\n",
        "    print('seed:', random_seed)\n",
        "    \n",
        "    torch.manual_seed(random_seed)\n",
        "    \n",
        "    model = BatchNormNet()\n",
        "    model.to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimiser = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    model, train_epoch_losses, train_epoch_accuracy, val_epoch_losses, val_epoch_accuracy, _,_ = run_training_and_validation(model, device, learning_rate, num_epochs, criterion, optimiser, train_dataloader, val_dataloader, metrics = False, manual_lr_schedule=False, plot=True)\n",
        "    epoch_train_losses_by_run.append(train_epoch_losses)\n",
        "    epoch_val_losses_by_run.append(val_epoch_losses)\n",
        "    epoch_train_accuracies_by_run.append(train_epoch_accuracy)\n",
        "    epoch_val_accuracies_by_run.append(val_epoch_accuracy)\n",
        "    \n",
        "    test_loss, test_accuracy, report = run_testing(model, device, criterion, test_dataloader)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    reports.append(report)\n",
        "    \n",
        "average_train_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_train_losses_by_run)]\n",
        "average_val_losses = [sum(epoch_losses) / len(epoch_losses) for epoch_losses in zip(*epoch_val_losses_by_run)]\n",
        "average_train_accuracies = [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_train_accuracies_by_run)]\n",
        "average_val_accuracies =  [sum(epoch_accuracies) / len(epoch_accuracies) for epoch_accuracies in zip(*epoch_val_accuracies_by_run)]\n",
        "average_test_loss = sum(test_losses)/len(test_losses)\n",
        "average_test_accuracy = sum(test_accuracies)/len(test_accuracies)\n",
        "\n",
        "averaged_results['bn'] = {'seeds':random_seeds,'av_train_losses': average_train_losses,\n",
        "                                    'av_val_losses': average_val_losses,\n",
        "                                    'av_train_acc': average_train_accuracies,\n",
        "                                    'av_val_acc': average_val_accuracies,\n",
        "                                    'all_train_losses':epoch_train_losses_by_run,\n",
        "                                    'all_val_losses': epoch_val_losses_by_run,\n",
        "                                    'all_train_accuracies': epoch_train_accuracies_by_run,\n",
        "                                    'all_val_accuracies': epoch_val_accuracies_by_run,\n",
        "                                    'all_test_losses':test_losses, \n",
        "                                    'all_test_accuracies':test_accuracies,\n",
        "                                    'av_test_loss': average_test_loss,\n",
        "                                    'av_test_accuracy':average_test_accuracy}\n",
        "print('average for ')\n",
        "plot_single_train_val_smoothed(average_train_losses,average_val_losses,average_train_accuracies,average_val_accuracies, num_epochs, smoothing_window=3, title=f'BATCH NORM MODEL')\n",
        "\n",
        "    \n",
        "if save_experiment:\n",
        "    with open(path_to_save, 'w') as file:\n",
        "        json.dump(averaged_results, file, indent=4)  # 'indent' makes the output formatted and easier to read\n",
        "\n",
        "batch_norm = 'run_data/batch_norm/batch_norm_50_epochs_LR_0.05.json'\n",
        "plot_all_models_performance_from_disk(batch_norm, enforce_axis=True)\n",
        "plot_performance_comparison_from_file(batch_norm, enforce_axis=True)\n",
        "display_accuracy_heatmap(batch_norm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pRSqHgx0tvD"
      },
      "source": [
        "# Conclusions and Discussion (instructions) - 25 MARKS <ignore>\n",
        "In this section, you are expected to:\n",
        "* briefly summarise and describe the conclusions from your experiments (8 MARKS).\n",
        "* discuss whether or not your results are expected, providing scientific reasons (8 MARKS).\n",
        "* discuss two or more alternative/additional methods that may enhance your model, with scientific reasons (4 MARKS). \n",
        "* Reference two or more relevant academic publications that support your discussion. (4 MARKS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v02f4sbyPuLh"
      },
      "source": [
        "These experiments demonstrated some of the fundamental properties of aritficial nueral networks.\n",
        "\n",
        "Experiment one demonstrated the effect that the learning can have on a models ability to fit to training data, and the impact that this has on generalisation. It showed that too high a LR could lead to coarse updates that lead to instability and variability in performance and and in ability to get to the true optimal minimial loss. It also showed that low LRs lead to slow progress but more close fit to training data.\n",
        "\n",
        "It was shown that a LR scheduler can balance these properties and lead to quick learning with more fine grained accuracy in later stages of training. However, this did not translate to significant benefits in validation and test performance, although there was some.\n",
        "\n",
        "Experiment two demonstrated the regularisation effect of dropout both in regular trianing and in a trasnfer learning scenario. It was shown to have a signficant impact on the generalisation gap reducing it as the rate increased and reducing the validation loss signifcantly. It was also found to have a profound regularising effect in the transfer learning example. Although it did lead to improvements in performance this were fairly small. \n",
        "\n",
        "Experiment 3 demonstrated clearly the powerful impact of batch normalisation on the propogation of gradient backwards through they layers of a neural network. The stark contract in average gradients arriving in the early layers in the intial impacts was striking. The impact it had on model performance however was perhaps a bit dissapointing, but I believe can be understood.\n",
        "\n",
        "The results in experiments one and two are very much to be expected.\n",
        "\n",
        "Learning rates are known to have a significant impact on the training dynamics and convergence of neural networks. High LRs can lead to overshooting the optimal solution and oscillations around the minimum, while low LRs result in slow convergence but more stable updates. The use of LR schedulers, such as reducing the LR over time, allows for faster initial convergence while fine-tuning the model in later stages. This is consistent with the observations in experiment one. \n",
        "\n",
        "The regularization effect of dropout is also well-established. Dropout introduces noise and stochasticity into the network by randomly dropping activation, preventing over-reliance on individual neurons and promoting more robust representations. This leads to improved generalization and reduced overfitting, as demonstrated in experiment two. Experiment 2 also demonstrates the impact that this can have on a networks ability to fit to data - with a reduced performance on the training data going along with the increased accuracy. \n",
        "\n",
        "The results of experiment three, however, were less expected. Although the gradient flow analysis clearly showed the powerful effect of batch normalization on the propagation of gradients backward through the layers of the neural network, the batch-normalized model performed quite poorly on unseen data, with the generalisation performance on the test set being really quite poor. \n",
        "\n",
        "This result is was surprising as one of the benefits of batch normalization has been shown to be its regularization effect [1], [6] and I was expecting it to *reduce* overfitting, but it did not. However, my understanding is that one of the headline benefits of batch normalisation it how it can speed up learning due to this early propogation of gradient to all layers (as was seen here). Given this more careful consideration needs to be given to other hyperparamaters which should compliment this drastic change. In these experiments all hyperparameters were fixed other than those being investigated.\n",
        "\n",
        "There are a number of approaches I would explore to enhance the model. As well as experimenting to find more complimentary hyperparamaters for use with batch normalisation, I would use a more advanced optimizer such as the 'Adam' optimiser [2] which is near uniquitous and recommended for most cases as \"one of the more robust and effective optimization algorithms to use in deep learning\" [15].\n",
        "\n",
        "I would also like to experiment with skip connections which were introduced in more recent and successful architectures, most noteably resnets [4]. I would also try experimenting with augmenting the data in the dataset by trying some of the techniques to increase the size and diversity of the training data, such as random rotations, flips, crops, and color jittering. Data augmentation can help improve the model's ability to generalize by exposing it to a wider range of variations and reducing overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References (instructions) <ignore>\n",
        "Use the cell below to add your references. A good format to use for references is like this:\n",
        "\n",
        "[AB Name], [CD Name], [EF Name] ([year]), [Article title], [Journal/Conference Name] [volume], [page numbers] or [article number] or [doi]\n",
        "\n",
        "Some examples:\n",
        "\n",
        "JEM Bennett, A Phillipides, T Nowotny (2021), Learning with reinforcement prediction errors in a model of the Drosophila mushroom body, Nat. Comms 12:2569, doi: 10.1038/s41467-021-22592-4\n",
        "\n",
        "SO Kaba, AK Mondal, Y Zhang, Y Bengio, S Ravanbakhsh (2023), Proc. 40th Int. Conf. Machine Learning, 15546-15566\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<ignore> \n",
        "\n",
        "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n",
        "\n",
        "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016.\n",
        "\n",
        "Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. CoRR, abs/1608.06993, 2016.\n",
        "\n",
        "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n",
        "\n",
        "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):8490, 2017.\n",
        "\n",
        "Nguyen Huu Phong and Bernardete Ribeiro. Rethinking recurrent neural networks and other improvements for image classification. CoRR, abs/2007.15161, 2020.\n",
        "\n",
        "Pytorch Foundation. CrossEntropyLoss - PyTorch 2.3 documentation, https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html (accessed May 12, 2024). \n",
        "\n",
        "Pytorch Foundation. LogSoftmax - PyTorch 2.3 documentation, https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax (accessed May 12, 2024). \n",
        "\n",
        "Pytorch Foundation. NLLLoss - PyTorch 2.3 documentation, https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss (accessed May 12, 2024). \n",
        "\n",
        "Pytorch Foundation. SGD - PyTorch 2.3 documentation, https://pytorch.org/docs/stable/generated/torch.optim.SGD.html (accessed May 12, 2024). \n",
        "\n",
        "Pytorch Foundation. datasets - PyTorch 2.3 documentation, https://pytorch.org/vision/0.8/datasets.html (accessed May 12, 2024). \n",
        "\n",
        "P Kingma Diederik. Adam: A method for stochastic optimization. (No Title), 201 Available: https://d2l.ai/ (accessed May 12, 2024).\n",
        "\n",
        "Johan Bjorck, Carla P. Gomes, and Bart Selman. Understanding batch normalization. CoRR, abs/1806.02375, 2018.\n",
        "\n",
        "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n",
        "\n",
        "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016.\n",
        "\n",
        "Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. CoRR, abs/1608.06993, 2016.\n",
        "\n",
        "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. \n",
        "\n",
        "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. \n",
        "\n",
        "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of theACM, 60(6):8490, 2017.\n",
        "\n",
        "Nguyen Huu Phong and Bernardete Ribeiro. Rethinking recurrent neural networks and other improvements for image classification. CoRR, abs/2007.15161, 2020.\n",
        "\n",
        "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Bjorck, J., Gomes, C. P., Selman, B. (2018), Understanding batch normalization, CoRR abs/1806.02375.\n",
        "\n",
        "2. Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.698\n",
        "\n",
        "3. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020), An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929.\n",
        "\n",
        "4. He, K., Zhang, X., Ren, S., Sun, J. (2016), Deep residual learning for image recognition, In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778.\n",
        "\n",
        "5. Huang, G., Liu, Z., Weinberger, K. Q. (2016), Densely connected convolutional networks, CoRR abs/1608.06993.\n",
        "\n",
        "6. Ioffe, S., Szegedy, C. (2015), Batch normalization: Accelerating deep network training by reducing internal covariate shift, CoRR abs/1502.03167.\n",
        "\n",
        "7. Krizhevsky, A., Hinton, G., et al. (2009), Learning multiple layers of features from tiny images.\n",
        "\n",
        "8. Krizhevsky, A., Sutskever, I., Hinton, G. E. (2017), Imagenet classification with deep convolutional neural networks, Communications of theACM 60(6), 8490.\n",
        "\n",
        "9. Pytorch Foundation (2023), CrossEntropyLoss - PyTorch 2.3 documentation, https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html (accessed May 12, 2024).\n",
        "\n",
        "10. Pytorch Foundation (2023), datasets - PyTorch 2.3 documentation, https://pytorch.org/vision/0.8/datasets.html (accessed May 12, 2024).\n",
        "\n",
        "11. Pytorch Foundation (2023), LogSoftmax - PyTorch 2.3 documentation, https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax (accessed May 12, 2024).\n",
        "\n",
        "12. Pytorch Foundation (2023), NLLLoss - PyTorch 2.3 documentation, https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss (accessed May 12, 2024).\n",
        "\n",
        "13. Pytorch Foundation (2023), SGD - PyTorch 2.3 documentation, https://pytorch.org/docs/stable/generated/torch.optim.SGD.html (accessed May 12, 2024).\n",
        "\n",
        "14. Simonyan, K., Zisserman, A. (2014), Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556.\n",
        "\n",
        "15. Zhang, A., Lipton, Z. C., Li, M., & Smola, A. J. (2020). Dive into Deep Learning. Retrieved from https://d2l.ai/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
